<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>SRF January Phase and Range Model Build for Keras – jumper</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-eeb6d1c97efeeb0cfaf56837413d2c8e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">jumper</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../projects.html"> 
<span class="menu-text">projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html"> 
<span class="menu-text">publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> 
<span class="menu-text">blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/JumperAMC"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/andy-collins-27946734/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">SRF January Phase and Range Model Build for Keras</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This notebook is an update of all the code I have been compiling in January 2023 for SRF/Roke. The aim is to train an ML model to recognise 5 different settings on the SRF box.</p>
<p>I have included both phase and range information and flattened it in the original list to make compiling the model easier.</p>
<p>I took 100 scans of each of the test positions - so this is a very basic system but extensible and fine for demo purposes.</p>
<p>Classifiers are as follows: * 0 = Control (box as is) * 1 = A spanner placed in the middle of the box (head down) * 2 = Little gap in box opening - spanner wedged in middle thin ways * 3 = Big gap in box opening - spanner wedged in width ways * 4 = Human hand on middle of box</p>
<section id="capture-data-for-multiple-labels-to-build-a-data-array" class="level2">
<h2 class="anchored" data-anchor-id="capture-data-for-multiple-labels-to-build-a-data-array">Capture data for multiple labels to build a data array:</h2>
<section id="this-will-overwrite-the-current-data.npy-file-so-take-care" class="level3">
<h3 class="anchored" data-anchor-id="this-will-overwrite-the-current-data.npy-file-so-take-care">!!This will overwrite the current ‘data.npy’ file so take care!!</h3>
<p>The code below will build an array for you to use in training the machine learning model. You will need to assign an integer and lebel to each class and tell the programme how many runs of each class you want to do.</p>
<p>As a precaution I reccomend you delete the existing “data.npy” file (after backing it up somewhere else safely).</p>
<p>IMPORTANT - in the code below it will stop after all the runs have been done but you need to MANUALLY HIT STOP in the cell menu toolbar in order for the file to compile and save properly. It’s probably a TODO to fix that but I am not going to mess about with it now it works!</p>
<div id="cell-3" class="cell" data-scrolled="true">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Looks OK as of 15/1/24 but something went haywire and the whole thing needed to calm down!</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> SiRad</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_measurement_and_save(num_arrays):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    MySiRad <span class="op">=</span> SiRad.SiRadDriver()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize an empty list to store arrays</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        data_arrays <span class="op">=</span> []</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_arrays):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>                label <span class="op">=</span> <span class="bu">input</span>(<span class="st">"Enter the label for the training data: "</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">KeyboardInterrupt</span>:</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Close SiRad connection on keyboard interrupt</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>                MySiRad.close()</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Measurement and save interrupted."</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>                save_data_to_file(data_arrays)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_arrays):  <span class="co"># Corrected loop structure</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">'Taking fingerprint'</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Run a measurement</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>                MySiRad.measure()</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Get measurement data</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>                fingerprint <span class="op">=</span> MySiRad.RangeData_dB</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>                fingerprint2 <span class="op">=</span> MySiRad.PhaseData_degrees</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>                <span class="co">#print(fingerprint) # Test line to make sure fingerprint is capturing!</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Combine both readings into the same array along with the label</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>                largearray <span class="op">=</span> np.array([fingerprint <span class="op">+</span> fingerprint2, label])  <span class="co"># label is now a separate element</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Append the array to the list</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>                data_arrays.append(largearray)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Print array shape</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(largearray.shape)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Sleep for 0.2 seconds</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>                time.sleep(<span class="fl">0.1</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load existing data (if any)</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>            existing_data <span class="op">=</span> np.load(<span class="st">"data.npy"</span>, allow_pickle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>            data_arrays.extend(existing_data.tolist())</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">pass</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save all arrays to file</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        save_data_to_file(data_arrays)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">finally</span>:</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Close SiRad connection regardless of exceptions</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>        MySiRad.close()</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_data_to_file(data_arrays):</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save data to file, overwriting existing file</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    np.save(<span class="st">"data.npy"</span>, np.array(data_arrays))</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the number of arrays from the user</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    num_arrays <span class="op">=</span> <span class="bu">int</span>(<span class="bu">input</span>(<span class="st">"Enter the number of arrays to generate for each label: "</span>))</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run the measurement and save loop</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    run_measurement_and_save(num_arrays)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="training-the-model-in-tensorflow" class="level2">
<h2 class="anchored" data-anchor-id="training-the-model-in-tensorflow">Training the model in Tensorflow:</h2>
<div id="cell-5" class="cell" data-outputid="e95887a1-349c-424d-8598-28e0e2acfc65">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TensorFlow and tf.keras</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper libraries</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tf.__version__)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2024-01-15 16:33:32.634371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>2.12.0</code></pre>
</div>
</div>
<p>This is the data array from my other 2023 notebook. I can take a slice and plot it. This is stored in the “TF for Oil and Gas folder” top level.</p>
<div id="cell-7" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.load(<span class="st">"data.npy"</span>, allow_pickle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A quick check to make sure I have the right data by slicing out a scan and plotting it. You can see this is not yet normalised.</p>
<div id="cell-9" class="cell" data-outputid="f2b0faee-4c04-4910-ae39-5b393b0ffa22">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data.shape)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> [arr[<span class="dv">0</span>][:<span class="dv">512</span>] <span class="cf">for</span> arr <span class="kw">in</span> data]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(2500, 2)</code></pre>
</div>
</div>
<div id="cell-10" class="cell" data-outputid="651c7fc9-a6c8-4791-881b-fffc1f878b90">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>plt.plot(result[<span class="dv">3</span>])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2023jansrftrainingmodelWORKING_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Some post processing is required as I need to reshape the array so it is suitable for Keras.</p>
<div id="cell-12" class="cell" data-scrolled="true" data-outputid="0406fc4b-13f1-4655-a230-0bc5411d6bdd">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>data.shape</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([[list([14, 28, 30, 23, 12, 30, 38, 35, 43, 49, 75, 76, 43, 24, 22, 32, -9, 38, 37, 23, 44, 41, 9, 38, 25, 3, 18, 27, 21, -10, -1, 27, 29, 10, -1, 0, 22, -14, 23, 5, 19, 25, 18, -49, 1, 17, 21, 9, 20, 26, 17, -1, 24, 8, 4, 21, 12, 26, 17, 10, 13, 10, 12, -3, 17, 25, 7, 3, 20, 24, 5, 1, -3, -9, 0, 1, -8, -1, -18, -4, 3, 2, 2, 5, -12, 4, -2, -9, -6, 1, 4, 2, -23, -14, -9, -5, -34, -14, -8, -5, 2, -6, -18, -9, -14, -10, -25, -26, -14, -13, -27, -15, -2, -8, -22, -23, -10, -29, -14, -12, -28, -28, -14, -11, -38, -37, -28, -42, -17, -14, -30, -36, -43, -33, -38, -37, -21, -20, -31, -35, -25, -18, -33, -42, -27, -30, -29, -22, -31, -25, -59, -33, -33, -38, -38, -29, -41, -62, -43, -52, -41, -55, -35, -33, -30, -40, -38, -42, -55, -63, -33, -42, -37, -53, -71, -36, -49, -42, -42, -78, -69, -64, -49, -52, -62, -57, -63, -53, -54, -67, -75, -48, -64, -56, -57, -45, -36, -53, -43, -62, -41, -72, -55, -60, -45, -56, -50, -73, -52, -75, -54, -68, -61, -66, -57, -51, -67, -63, -71, -77, -72, -59, -61, -66, -63, -61, -73, -70, -59, -85, -69, -71, -64, -78, -66, -63, -70, -64, -70, -75, -83, -59, -56, -76, -67, -67, -73, -65, -79, -80, -63, -67, -67, -59, -63, -52, -54, -180, 40, -22, -180, -180, 40, -54, 40, -180, -168, 40, -180, -180, -79, -180, -180, 40, -140, 40, -180, -124, -59, 40, -171, 28, 40, -180, 40, -96, -180, 40, -110, 40, -180, -180, 40, 40, -180, 40, 40, -110, 40, -136, -29, -180, 40, -34, -26, -180, 40, 40, -180, 40, 40, -180, 40, -66, -180, 40, 4, -180, -60, 40, -180, 40, -144, -180, -44, -180, 26, -180, -28, -144, -180, 40, -180, 32, -180, -180, 40, -180, -150, 40, -154, -180, -45, -112, 40, -180, 40, -13, -180, 40, -180, 40, -180, -130, 40, -180, 40, -180, 40, 25, -180, 40, -108, 40, -180, 40, -180, -180, 40, -180, -180, 40, 40, -180, -39, -180, 40, 40, -31, -180, -180, 40, 40, -180, -96, 40, -180, -101, -55, 40, 36, 40, -133, 40, -160, 40, -128, 40, -66, -161, -180, 40, -5, -180, 40, -163, 40, 40, -180, -180, 40, -28, -180, -16, 40, -125, -180, -5, -26, 40, -180, 40, 37, -180, -180, -180, 40, -180, -102, 40, 40, -180, 40, 10, -180, -180, -20, 40, -180, 40, -82, 40, -176, 40, -180, -180, -5, 40, -180, -180, -180, -17, 40, -59, -180, -8, 40, -180, -102, -180, 40, -180, -52, -132, -180, 40, 40, -180, -180, 33, -180, 40, -180, -180, 40, -63, -180, -54, 40, -180, -4, -180, -138, -17, 40, -180, -59, -48, -180, 40, -175, 40, -180, -180, 40, -180, -105, 40, -69, -180, 40, -67, -180, 40, -117, 40, -180, -52, 40, -180, -125, 40]),
        '0'],
       [list([14, 28, 30, 23, 12, 30, 38, 35, 43, 49, 75, 76, 43, 24, 22, 32, -9, 38, 37, 23, 44, 41, 9, 38, 25, 3, 18, 27, 21, -10, -1, 27, 29, 10, -2, 1, 22, -14, 23, 6, 19, 25, 18, -55, 1, 17, 21, 9, 20, 26, 17, -1, 24, 8, 4, 21, 12, 26, 17, 10, 13, 10, 12, -3, 17, 25, 7, 3, 20, 24, 5, 1, -3, -10, 0, 1, -8, -1, -18, -4, 3, 2, 2, 5, -12, 4, -2, -9, -6, 1, 4, 2, -23, -15, -9, -5, -34, -14, -8, -5, 2, -6, -18, -9, -14, -10, -24, -27, -14, -13, -26, -15, -2, -8, -22, -23, -10, -29, -14, -12, -28, -28, -13, -10, -39, -37, -28, -43, -16, -14, -30, -36, -41, -33, -39, -37, -20, -20, -30, -34, -25, -19, -33, -42, -27, -30, -29, -22, -31, -25, -59, -33, -33, -37, -39, -29, -41, -63, -44, -52, -40, -55, -35, -33, -30, -39, -38, -41, -54, -66, -33, -41, -36, -54, -72, -36, -49, -42, -41, -75, -73, -62, -48, -51, -63, -55, -67, -54, -55, -68, -75, -48, -64, -58, -53, -44, -36, -53, -44, -63, -41, -66, -56, -63, -47, -58, -50, -73, -52, -73, -54, -71, -63, -67, -58, -52, -68, -63, -76, -70, -70, -58, -61, -64, -61, -62, -72, -70, -61, -82, -69, -67, -66, -71, -69, -63, -73, -65, -71, -72, -76, -58, -57, -81, -69, -69, -72, -62, -77, -80, -64, -65, -68, -60, -59, -52, -54, -180, 40, -22, -180, -180, 40, -54, 40, -180, -168, 40, -180, -180, -79, -180, -180, 40, -140, 40, -180, -124, -60, 40, -171, 29, 40, -180, 40, -97, -180, 40, -110, 40, -180, -180, 40, 40, -180, 40, 40, -109, 40, -102, -30, -180, 40, -34, -25, -180, 40, 40, -180, 40, 40, -180, 40, -65, -180, 40, 5, -180, -59, 40, -180, 40, -144, -180, -43, -180, 28, -180, -26, -143, -180, 40, -180, 33, -180, -180, 40, -180, -151, 40, -152, -180, -44, -111, 40, -180, 40, -13, -180, 40, -180, 40, -180, -126, 40, -180, 40, -180, 40, 26, -180, 40, -102, 40, -180, 40, -178, -180, 40, -180, -180, 40, 40, -180, -38, -180, 40, 40, -31, -180, -180, 40, 40, -180, -94, 40, -180, -99, -63, 40, 40, 40, -131, 40, -161, 40, -129, 40, -61, -172, -180, 40, 0, -180, 40, -164, 40, 40, -180, -180, 40, -26, -180, -7, 40, -121, -180, 9, -25, 40, -180, 40, 40, -170, -180, -180, 40, -180, -94, 40, 40, -180, 40, -3, -180, -9, -44, 40, -180, 40, -107, 40, -116, 40, -180, -180, -23, 40, -180, -180, -180, -49, 40, -59, -180, -19, 40, -180, -105, -180, 40, -180, -64, -176, -180, 13, 40, -180, -180, 14, -175, 40, -180, -180, 40, 33, -180, -59, 40, -180, 7, -180, -106, 40, 40, -180, -129, -49, 40, -180, -180, 40, -180, -180, 40, 40, -151, 40, -69, -180, -180, -60, -180, 40, -146, 40, -180, -58, 40, -180, -102, 40]),
        '0'],
       [list([14, 28, 30, 23, 12, 30, 38, 35, 43, 49, 75, 76, 43, 24, 22, 32, -9, 38, 38, 24, 44, 41, 9, 38, 25, 3, 18, 27, 21, -10, -1, 26, 29, 10, -3, 1, 22, -12, 23, 6, 19, 25, 18, -56, 1, 17, 21, 9, 20, 26, 17, -1, 24, 8, 4, 21, 12, 26, 17, 10, 13, 10, 12, -3, 17, 25, 7, 3, 20, 24, 5, 1, -3, -10, 0, 1, -8, -1, -18, -4, 3, 1, 2, 5, -12, 4, -2, -8, -6, 1, 4, 2, -23, -15, -8, -5, -34, -14, -8, -5, 2, -7, -18, -9, -14, -10, -24, -27, -14, -13, -26, -15, -2, -8, -22, -23, -11, -29, -14, -13, -28, -28, -14, -11, -38, -38, -28, -42, -16, -14, -29, -36, -41, -34, -39, -38, -21, -20, -31, -35, -25, -19, -34, -41, -26, -30, -28, -22, -31, -25, -60, -34, -33, -38, -39, -29, -41, -65, -44, -52, -41, -55, -35, -33, -30, -38, -39, -41, -54, -66, -33, -43, -36, -53, -71, -36, -50, -41, -42, -84, -71, -64, -50, -53, -61, -56, -67, -54, -56, -68, -77, -49, -63, -59, -54, -46, -37, -56, -42, -63, -41, -67, -56, -66, -47, -50, -49, -68, -52, -72, -54, -72, -63, -68, -58, -53, -70, -65, -77, -69, -70, -57, -61, -65, -58, -61, -76, -70, -61, -93, -64, -70, -67, -72, -64, -63, -73, -66, -74, -69, -79, -59, -57, -81, -67, -70, -71, -64, -73, -77, -65, -64, -73, -60, -59, -50, -55, -180, 40, -22, -180, -180, 40, -54, 40, -180, -168, 40, -180, -180, -79, -180, 40, 40, -140, 40, -180, -124, -59, 40, -169, 32, 40, -180, 40, -94, -180, 40, -109, 40, -180, -180, 40, 40, -180, 40, 40, -108, 40, -151, -28, -180, 40, -33, -25, -180, 40, 40, -180, 40, 40, -180, 40, -65, -180, 40, 6, -180, -59, 40, -180, 40, -143, -180, -43, -180, 29, -180, -26, -142, -180, 40, -180, 33, -180, -180, 40, -180, -151, 40, -149, -180, -43, -111, 40, -180, 40, -13, -180, 40, -180, 40, -180, -123, 40, -180, 40, -180, 40, 29, -180, 40, -97, 40, -180, 40, -174, -180, 40, -180, -180, 40, 40, -180, -38, -180, 40, 40, -27, -180, -180, 40, 40, -180, -91, 40, -180, -102, -46, 40, 40, 40, -132, 40, -162, 40, -125, 40, -54, -155, -180, 40, 4, -180, 40, -161, 40, 40, -180, -180, 40, -19, -180, 2, 40, -123, -180, -28, -23, 40, -180, 40, 40, -173, -180, -180, 40, -180, -101, 40, 40, -180, 40, 7, -180, -12, -29, 40, -180, 40, -58, 40, -177, 40, -180, -180, 40, 40, -180, -180, -180, -46, 40, -33, -180, 26, 40, 40, -105, -180, 28, -180, 13, -175, -180, 3, 40, -180, -180, 40, -179, 40, -180, -180, 40, -29, -180, -54, 40, -180, -18, -180, -150, 18, -180, -180, -63, -55, -180, 40, -180, 40, 40, -180, 40, 7, -166, 40, -50, -180, -180, -28, -180, 40, -116, 40, -180, -57, 40, -180, -122, 40]),
        '0'],
       ...,
       [list([17, 28, 30, 23, 14, 28, 37, 34, 41, 50, 75, 76, 43, 29, 22, 28, 7, 37, 35, 7, 45, 44, 20, 36, 16, 17, 14, 25, 18, -14, 1, 22, 20, 7, 5, 6, 22, -6, 29, 27, -12, 26, 20, -2, 6, 21, 28, 14, 26, 24, 20, 10, 27, -19, 9, 14, 8, 19, 15, 12, 19, 15, 6, 19, 18, 21, 21, 4, 19, 17, 1, 0, -7, -14, 7, -2, 1, 8, -11, 0, 9, 0, -39, -16, -10, -3, -10, -3, -17, -2, 1, 2, -2, -4, -6, 9, -2, -45, -25, -29, -5, -24, -4, -9, -27, -45, -37, -15, -7, -9, -12, -10, -1, -18, -12, -15, -20, -49, -4, -11, -35, -45, -21, -12, -43, -17, -31, -24, -67, -25, -29, -24, -25, -24, -30, -24, -25, -43, -40, -41, -48, -25, -20, -34, -41, -41, -24, -44, -54, -36, -23, -34, -30, -16, -22, -40, -33, -33, -44, -38, -39, -49, -44, -35, -33, -33, -50, -46, -60, -42, -35, -40, -54, -34, -48, -45, -33, -35, -39, -42, -53, -50, -43, -47, -58, -68, -54, -44, -51, -37, -53, -61, -50, -53, -51, -46, -55, -49, -76, -41, -47, -60, -43, -47, -56, -63, -44, -39, -45, -50, -64, -60, -56, -73, -61, -69, -66, -52, -49, -59, -51, -64, -55, -71, -64, -64, -70, -78, -80, -66, -80, -53, -51, -61, -83, -64, -62, -75, -55, -69, -68, -67, -58, -73, -72, -59, -71, -63, -68, -62, -61, -71, -68, -87, -57, -54, -60, -180, 40, -26, -180, -180, 40, -53, 40, -180, -168, 40, -180, -180, -49, -180, -180, 40, -123, 40, 40, -136, -3, 40, -132, 22, -180, -179, 40, -64, -49, 40, -108, 40, -173, -180, 40, 40, -180, 40, -180, -119, 40, 40, -12, -168, 40, -107, 40, -180, 40, 40, -121, -180, 40, -180, 40, -26, -180, -180, 40, -180, -51, 40, -177, 40, -162, 40, 2, -180, 40, -104, -11, -28, -180, 40, -180, 40, -180, -180, 40, -180, -180, 40, -55, -180, 40, -99, 40, -153, -180, 40, -47, -180, -180, 40, -119, -180, -180, -180, 40, -180, -180, 40, -158, 40, 40, -180, 40, -180, -64, -180, 40, -180, -57, -180, 40, -31, -2, -180, -180, 40, 40, -180, 40, 40, -180, 21, -166, -180, -21, -180, -28, 40, -180, -165, 40, 40, -139, -180, -16, 40, -93, -180, -180, -180, 40, -115, -70, -168, 40, -55, 40, -153, 40, 40, -63, -180, 40, 40, -73, -180, 40, -180, -67, -180, 34, 40, -31, -180, 40, -180, -180, 40, -170, -106, -180, 40, -180, 40, -118, 40, -180, 40, 37, 40, -180, -58, -127, -180, 40, 1, -180, 40, -119, 40, -180, -180, 40, 40, -180, -180, 40, -34, -180, -180, -180, 15, -180, 40, 40, -180, 40, -180, -166, 40, 40, -180, 40, 40, -163, -180, 40, 40, -58, -180, 14, 40, 40, -180, -180, 40, -180, 40, -163, -75, -180, 40, -2, -180, -180, -180, 40, -180, 40, -170, -180, 40, -180, 40, -180, -180, 21, -180, -180, 40]),
        '4'],
       [list([17, 28, 30, 23, 13, 28, 37, 34, 41, 50, 75, 76, 43, 29, 22, 28, 7, 36, 35, 7, 45, 44, 20, 36, 16, 17, 14, 26, 19, -14, 1, 22, 20, 8, 5, 6, 22, -6, 29, 27, -12, 26, 20, -2, 6, 21, 28, 14, 26, 24, 20, 10, 27, -19, 9, 14, 8, 19, 15, 12, 19, 15, 6, 19, 18, 21, 21, 3, 19, 17, 1, 0, -7, -14, 7, -2, 1, 8, -11, 0, 9, 0, -38, -16, -9, -3, -11, -3, -17, -2, 1, 2, -2, -5, -6, 9, -2, -48, -25, -29, -5, -24, -4, -9, -27, -46, -37, -14, -6, -9, -12, -10, -1, -18, -12, -14, -20, -49, -4, -11, -35, -45, -22, -13, -43, -17, -30, -24, -66, -24, -29, -23, -25, -24, -30, -24, -25, -44, -40, -41, -49, -24, -19, -34, -42, -42, -25, -44, -55, -36, -23, -35, -31, -16, -22, -40, -33, -32, -43, -38, -38, -49, -43, -34, -33, -34, -50, -48, -60, -42, -36, -40, -53, -34, -48, -46, -32, -33, -38, -41, -54, -50, -43, -47, -61, -66, -54, -45, -49, -37, -54, -57, -50, -52, -51, -46, -53, -50, -79, -40, -47, -61, -45, -47, -53, -62, -43, -40, -46, -52, -63, -61, -55, -69, -58, -66, -68, -53, -49, -58, -52, -67, -57, -73, -65, -64, -72, -84, -84, -67, -75, -53, -52, -62, -94, -65, -63, -73, -54, -65, -69, -71, -60, -73, -71, -62, -74, -64, -70, -61, -61, -68, -68, -70, -54, -51, -58, -180, 40, -26, -180, -180, 40, -53, 40, -180, -168, 40, -180, -180, -49, -180, -180, 40, -121, 40, 40, -136, -3, 40, -132, 25, -180, -180, 40, -73, -45, 40, -106, 40, -176, -180, 40, 40, -180, 40, -180, -119, 40, 40, -10, -167, 40, -106, 40, -180, 40, 40, -121, -180, 40, -180, 40, -26, -180, -180, 40, -180, -50, 40, -175, 40, -161, 40, 3, -180, 40, -102, -13, -25, -180, 40, -180, 40, -180, -180, 40, -180, -180, 40, -55, -180, 40, -94, 40, -150, -180, 40, -46, -180, -180, 40, -117, -180, -180, -180, 40, -180, -180, 40, -147, 40, 40, -180, 40, -180, -62, -180, 40, -180, -53, -180, 40, -6, -2, -180, -180, 40, 40, -180, 40, 40, -180, 26, -147, -180, -12, -180, -24, 40, -180, -162, 40, 40, -138, -180, -12, 40, -95, -180, -180, -180, 40, -101, -75, -161, -180, -49, 40, -152, 40, 40, -61, -180, 40, -180, -67, -180, 40, -180, -68, -180, 40, 40, -5, -180, 40, -180, -180, 40, -162, -110, -180, 40, -180, 40, -138, 40, -180, 40, 17, 40, -180, -57, -105, -180, 38, 35, -180, 40, -116, 40, -180, -180, 40, 40, -180, -180, 40, -11, -180, -180, 40, 1, -180, 40, 40, -180, 40, -180, -180, 40, 40, -180, 40, 31, -180, -180, 40, 40, -54, -180, 33, 40, 40, -144, -180, 40, -171, 40, 40, -176, -180, 40, 13, -180, -180, 40, 1, -180, 40, -180, -180, 40, -180, 40, -180, -180, -35, 40, -180, 40]),
        '4'],
       [list([17, 28, 30, 23, 13, 28, 37, 34, 41, 50, 75, 76, 43, 29, 22, 28, 7, 36, 35, 7, 45, 44, 20, 36, 16, 17, 14, 26, 19, -13, 1, 23, 20, 8, 5, 6, 22, -6, 29, 27, -12, 26, 20, -1, 6, 21, 28, 13, 26, 24, 20, 11, 27, -19, 9, 14, 8, 20, 15, 12, 19, 15, 6, 19, 18, 21, 21, 3, 19, 17, 1, 0, -7, -14, 7, -2, 1, 8, -11, 0, 9, 0, -38, -16, -10, -3, -10, -3, -17, -1, 1, 2, -2, -5, -7, 9, -2, -50, -25, -30, -5, -24, -4, -9, -27, -44, -37, -14, -6, -9, -13, -10, -1, -19, -11, -14, -20, -49, -4, -11, -35, -46, -21, -12, -43, -17, -31, -24, -62, -24, -29, -23, -24, -24, -30, -25, -26, -43, -40, -41, -49, -24, -19, -34, -39, -43, -25, -44, -53, -36, -22, -33, -30, -16, -22, -40, -33, -33, -43, -38, -38, -49, -43, -34, -33, -34, -52, -50, -55, -42, -36, -40, -54, -35, -48, -46, -33, -34, -38, -42, -55, -51, -43, -47, -58, -64, -54, -44, -49, -37, -55, -58, -50, -55, -53, -45, -54, -48, -80, -40, -48, -57, -44, -46, -54, -63, -46, -41, -46, -51, -63, -62, -58, -78, -59, -66, -69, -53, -48, -59, -54, -66, -57, -78, -66, -63, -66, -77, -86, -70, -75, -52, -54, -66, -84, -63, -61, -70, -52, -68, -66, -72, -61, -76, -71, -57, -73, -64, -67, -63, -60, -64, -70, -77, -56, -52, -58, -180, 40, -26, -180, -180, 40, -53, 40, -180, -168, 40, -180, -180, -49, -180, -180, 40, -121, 40, 40, -136, -3, 40, -132, 25, -180, -180, 40, -70, -45, 40, -106, 40, -175, -180, 40, 40, -180, 40, -180, -119, 40, 40, -10, -167, 40, -105, 40, -180, 40, 40, -121, -180, 40, -180, 40, -25, -180, -180, 40, -180, -49, 40, -175, 40, -161, 40, 2, -180, 40, -102, -12, -24, -180, 40, -180, 40, -180, -180, 40, -180, -180, 40, -55, -180, 40, -95, 40, -150, -180, 40, -44, -180, -180, 40, -120, -180, -180, -180, 40, -180, -180, 40, -152, 40, 40, -180, 40, -180, -60, -180, 40, -180, -52, -180, 40, -13, 1, -180, -180, 40, 40, -180, 40, 40, -180, 24, -134, -180, -12, -180, -25, 40, -180, -169, 40, 40, -133, -180, -17, 40, -92, -180, -180, -180, 40, -104, -54, -163, -180, -44, 40, -152, 40, 40, -63, -180, 40, 40, -70, -180, 40, -180, -63, -180, 40, 40, 1, -180, 40, -180, -180, 40, -174, -108, -180, 40, -180, 40, -139, 40, -180, 40, 33, 40, -180, -53, -95, -180, 36, 40, -180, 40, -144, 40, -180, -180, 40, 40, -180, -180, 40, -44, -180, -130, -180, 10, -180, 40, 10, -180, 40, -180, -180, -180, 40, -161, 40, 37, -180, -151, 40, 40, -25, -180, 28, -180, 40, -148, -180, 40, -163, 40, -125, -24, -180, 40, -23, -180, -180, -180, 36, 40, 40, -180, 40, 40, -180, 40, -180, -180, 17, 40, -180, 40]),
        '4']], dtype=object)</code></pre>
</div>
</div>
<p>I now need to extract the label and list items from that numpy array ready for recombining.</p>
<div id="cell-14" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the first column (containing the lists) and then slice each list to get the first 512 items</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>lists <span class="op">=</span> [arr[<span class="dv">0</span>][:<span class="dv">512</span>] <span class="cf">for</span> arr <span class="kw">in</span> data]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the second column (containing the labels)</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [arr[<span class="dv">1</span>] <span class="cf">for</span> arr <span class="kw">in</span> data]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 'lists' will contain the lists with the first 512 items</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 'labels' will contain the associated labels</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># I need to combine these labels and lists into a trainable array</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This will turn the lists and labels into proper numpy arrays for processing in the correct shape.</p>
<p><strong>It is important to note that the labels array needs to be changed from “U1” data format to “int64” in order to work with Keras!</strong> The code below does this for the labels array. The lists array is OK as is.</p>
<div id="cell-16" class="cell" data-outputid="3ad2477c-ef7d-4056-b296-4047ac45e3c8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>lists <span class="op">=</span> np.array(lists)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming labels is loaded as an array with inferred data type</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.array(labels, dtype<span class="op">=</span>np.int64)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(labels.shape)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(lists.shape)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(labels.dtype)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(lists.dtype)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(2500,)
(2500, 512)
int64
int64</code></pre>
</div>
</div>
<p>##&nbsp;Normalise the lists array:</p>
<p>The code below will take the lists array of scans and normalises it to ranges between 0 - 1. This is important as Tensorflow needs data to be normalised in order to work properly!</p>
<p>As a sanity check I take a slice from the original array and compare to the normalised one.</p>
<p>The code below takes a single array and converts it.</p>
<div id="cell-18" class="cell" data-outputid="6d42b79c-65cf-4319-c16b-f70cb7eda8d5">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming X_train[340] is your original array</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>original_array <span class="op">=</span> lists[<span class="dv">340</span>]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the array to np.int64 (if necessary)</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>new_list_item_array <span class="op">=</span> np.array(original_array, dtype<span class="op">=</span>np.int64)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Min-Max scaling</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>min_value <span class="op">=</span> np.<span class="bu">min</span>(new_list_item_array)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>max_value <span class="op">=</span> np.<span class="bu">max</span>(new_list_item_array)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>normalized_array <span class="op">=</span> (new_list_item_array <span class="op">-</span> min_value) <span class="op">/</span> (max_value <span class="op">-</span> min_value)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the original and normalized arrays</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>plt.plot(new_list_item_array)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Original Array'</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>plt.plot(normalized_array)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Normalized Array'</span>)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2023jansrftrainingmodelWORKING_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="normalise-entire-array-of-lists" class="level2">
<h2 class="anchored" data-anchor-id="normalise-entire-array-of-lists">Normalise entire array of lists:</h2>
<p>This code iterates over the entire lists array and normalises each scan within it:</p>
<div id="cell-20" class="cell" data-outputid="7fa54ef1-aa01-4fab-c477-9deba61f1140">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming lists is your 2D numpy array with shape (500, 512)</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># You can replace it with your actual array</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>lists2 <span class="op">=</span> lists  <span class="co"># Example random array</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize an empty array to store normalized arrays</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>normalized_arrays <span class="op">=</span> np.zeros_like(lists, dtype<span class="op">=</span>np.float64)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over each array in the 2D array and normalize</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(lists2.shape[<span class="dv">0</span>]):</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    original_array <span class="op">=</span> lists2[i]</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert the array to np.int64 (if necessary)</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    new_list_item_array <span class="op">=</span> np.array(original_array, dtype<span class="op">=</span>np.int64)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Min-Max scaling</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    min_value <span class="op">=</span> np.<span class="bu">min</span>(new_list_item_array)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    max_value <span class="op">=</span> np.<span class="bu">max</span>(new_list_item_array)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    normalized_array <span class="op">=</span> (new_list_item_array <span class="op">-</span> min_value) <span class="op">/</span> (max_value <span class="op">-</span> min_value)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store the normalized array in the new array</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    normalized_arrays[i, :] <span class="op">=</span> normalized_array</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the original and normalized arrays for a specific item (e.g., first item)</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>plt.plot(lists2[<span class="dv">0</span>, :])</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Original Array'</span>)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>plt.plot(normalized_arrays[<span class="dv">0</span>, :])</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Normalized Array'</span>)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2023jansrftrainingmodelWORKING_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-21" class="cell" data-outputid="cc64d20d-16ce-459c-8c62-127fe9ce8b3c">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>normalized_arrays.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>(2500, 512)</code></pre>
</div>
</div>
<p>The code above works for normalising the whole array! Now redo the model with the new normalized_array - change to lists!</p>
<div id="cell-23" class="cell" data-outputid="46a912f2-f474-48a5-9e88-a830898570d2">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>lists <span class="op">=</span> normalized_arrays</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>lists.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>(2500, 512)</code></pre>
</div>
</div>
</section>
<section id="build-the-model" class="level1">
<h1>Build the model:</h1>
<section id="build-train-and-test-the-model" class="level2">
<h2 class="anchored" data-anchor-id="build-train-and-test-the-model">Build, Train and Test the model</h2>
<p>This example creates a simple feedforward neural network with one input layer, two hidden layers, and one output layer. Adjust the architecture based on your specific requirements. The model is then compiled, trained, and evaluated. Finally, the trained model is saved.</p>
<div id="cell-27" class="cell" data-outputid="027d8504-3c6c-4436-d736-33c52abe8bb9">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dense</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.utils <span class="im">import</span> to_categorical</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 'lists' and 'labels' are your lists and labels arrays</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert lists and labels to numpy arrays</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>lists <span class="op">=</span> np.array(lists)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.array(labels)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and testing sets</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(lists, labels, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert labels to categorical one-hot encoding</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="bu">len</span>(np.unique(labels))</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> to_categorical(y_train, num_classes)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> to_categorical(y_test, num_classes)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple neural network model</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential()</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>model.add(Dense(<span class="dv">64</span>, input_shape<span class="op">=</span>(X_train.shape[<span class="dv">1</span>],), activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>model.add(Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>))</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>model.add(Dense(num_classes, activation<span class="op">=</span><span class="st">'softmax'</span>))</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>, optimizer<span class="op">=</span><span class="st">'adam'</span>, metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train, epochs<span class="op">=</span><span class="dv">10</span>, batch_size<span class="op">=</span><span class="dv">32</span>, validation_split<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the model on the test set</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>loss, accuracy <span class="op">=</span> model.evaluate(X_test, y_test)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Test Loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">, Test Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the trained model</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">'your_model-6.h5'</span>)  <span class="co"># Replace 'your_model.h5' with the desired file name</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10
57/57 [==============================] - 2s 8ms/step - loss: 0.4568 - accuracy: 0.8889 - val_loss: 0.0331 - val_accuracy: 1.0000
Epoch 2/10
57/57 [==============================] - 0s 4ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000
Epoch 3/10
57/57 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000
Epoch 4/10
57/57 [==============================] - 0s 3ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000
Epoch 5/10
57/57 [==============================] - 0s 5ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000
Epoch 6/10
57/57 [==============================] - 0s 5ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000
Epoch 7/10
57/57 [==============================] - 0s 4ms/step - loss: 9.5228e-04 - accuracy: 1.0000 - val_loss: 8.5184e-04 - val_accuracy: 1.0000
Epoch 8/10
57/57 [==============================] - 0s 4ms/step - loss: 7.3767e-04 - accuracy: 1.0000 - val_loss: 6.5189e-04 - val_accuracy: 1.0000
Epoch 9/10
57/57 [==============================] - 0s 4ms/step - loss: 5.7740e-04 - accuracy: 1.0000 - val_loss: 5.2372e-04 - val_accuracy: 1.0000
Epoch 10/10
57/57 [==============================] - 0s 3ms/step - loss: 4.7594e-04 - accuracy: 1.0000 - val_loss: 4.3213e-04 - val_accuracy: 1.0000
16/16 [==============================] - 0s 4ms/step - loss: 4.3928e-04 - accuracy: 1.0000
Test Loss: 0.00043927744263783097, Test Accuracy: 1.0
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 64)                32832     
                                                                 
 dense_1 (Dense)             (None, 32)                2080      
                                                                 
 dense_2 (Dense)             (None, 5)                 165       
                                                                 
=================================================================
Total params: 35,077
Trainable params: 35,077
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<p>Sanity check here that you have properly reshaped and reasonable sizes of arrays.</p>
<div id="cell-29" class="cell" data-outputid="ca7c7fc9-09c5-47bc-a338-3b841c18150c">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Shape of X_train:"</span>, X_train.shape)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Shape of y_train:"</span>, y_train.shape)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Shape of X_test:"</span>, X_test.shape)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Shape of y_test:"</span>, y_test.shape)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of X_train: (2000, 512)
Shape of y_train: (2000, 5)
Shape of X_test: (500, 512)
Shape of y_test: (500, 5)</code></pre>
</div>
</div>
<p>Sanity check that the data looks reaosnable:</p>
<div id="cell-31" class="cell" data-scrolled="true" data-outputid="6f278524-e031-4750-9712-415e345f1398">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a few samples from the training set</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Sample </span><span class="sc">{}</span><span class="st">: X = </span><span class="sc">{}</span><span class="st">, y = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(i<span class="op">+</span><span class="dv">1</span>, X_train[i], y_train[i]))</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a few samples from the testing set</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Sample </span><span class="sc">{}</span><span class="st">: X = </span><span class="sc">{}</span><span class="st">, y = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(i<span class="op">+</span><span class="dv">1</span>, X_test[i], y_test[i]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sample 1: X = [0.76953125 0.8125     0.8203125  0.79296875 0.7578125  0.80859375
 0.84765625 0.83203125 0.85546875 0.90234375 0.99609375 1.
 0.8671875  0.82421875 0.77734375 0.7890625  0.78125    0.859375
 0.84765625 0.65625    0.88671875 0.8828125  0.79296875 0.83203125
 0.73046875 0.76953125 0.734375   0.80859375 0.78515625 0.671875
 0.73046875 0.7890625  0.7578125  0.703125   0.71484375 0.7265625
 0.80859375 0.7109375  0.81640625 0.80859375 0.6875     0.78515625
 0.73828125 0.6953125  0.6796875  0.77734375 0.8125     0.7890625
 0.79296875 0.74609375 0.7578125  0.7265625  0.79296875 0.68359375
 0.7578125  0.74609375 0.75390625 0.703125   0.734375   0.71484375
 0.76171875 0.7578125  0.73828125 0.78515625 0.75       0.7578125
 0.7734375  0.703125   0.7578125  0.71484375 0.734375   0.734375
 0.65625    0.6953125  0.73828125 0.6875     0.6953125  0.6875
 0.68359375 0.70703125 0.74609375 0.73828125 0.6328125  0.65234375
 0.69140625 0.69921875 0.59765625 0.66796875 0.61328125 0.61328125
 0.67578125 0.73828125 0.6015625  0.6640625  0.703125   0.7421875
 0.703125   0.6484375  0.65234375 0.6875     0.66796875 0.65234375
 0.69140625 0.65625    0.60546875 0.66796875 0.6328125  0.6875
 0.703125   0.6796875  0.65625    0.68359375 0.703125   0.6328125
 0.66796875 0.66015625 0.64453125 0.57421875 0.6796875  0.5703125
 0.63671875 0.5625     0.640625   0.63671875 0.60546875 0.61328125
 0.6171875  0.6015625  0.58984375 0.5625     0.6015625  0.6171875
 0.58203125 0.6484375  0.62890625 0.5625     0.515625   0.59765625
 0.62109375 0.57421875 0.46484375 0.43359375 0.515625   0.53125
 0.53515625 0.6328125  0.62109375 0.5859375  0.51953125 0.5703125
 0.61328125 0.56640625 0.5390625  0.61328125 0.55859375 0.57421875
 0.53515625 0.5703125  0.5703125  0.46875    0.52734375 0.546875
 0.484375   0.5859375  0.5859375  0.4921875  0.48046875 0.546875
 0.5390625  0.515625   0.5390625  0.5078125  0.55078125 0.41796875
 0.484375   0.51953125 0.5390625  0.5703125  0.546875   0.5546875
 0.421875   0.4765625  0.5234375  0.55859375 0.453125   0.49609375
 0.40625    0.33984375 0.48828125 0.5625     0.5390625  0.484375
 0.515625   0.48828125 0.48828125 0.48828125 0.515625   0.4765625
 0.51953125 0.52734375 0.40625    0.51953125 0.53125    0.5390625
 0.49609375 0.48828125 0.5234375  0.515625   0.4453125  0.51953125
 0.48828125 0.5        0.5234375  0.515625   0.41796875 0.421875
 0.3828125  0.4296875  0.43359375 0.4921875  0.453125   0.46875
 0.47265625 0.421875   0.4765625  0.46484375 0.45703125 0.4296875
 0.44140625 0.46875    0.47265625 0.484375   0.50390625 0.47265625
 0.484375   0.49609375 0.453125   0.46875    0.40625    0.4609375
 0.484375   0.4296875  0.4765625  0.47265625 0.40234375 0.44921875
 0.41015625 0.40234375 0.37890625 0.4296875  0.453125   0.38671875
 0.39453125 0.41796875 0.41015625 0.4921875  0.4765625  0.
 0.859375   0.59765625 0.         0.         0.859375   0.5
 0.859375   0.         0.0546875  0.859375   0.         0.
 0.48828125 0.         0.         0.859375   0.28515625 0.
 0.859375   0.16796875 0.8125     0.859375   0.46484375 0.80078125
 0.         0.04296875 0.859375   0.453125   0.59765625 0.859375
 0.2421875  0.859375   0.15625    0.         0.859375   0.
 0.         0.859375   0.         0.359375   0.859375   0.
 0.859375   0.25390625 0.859375   0.24609375 0.859375   0.
 0.         0.859375   0.4296875  0.         0.859375   0.
 0.77734375 0.828125   0.         0.         0.859375   0.
 0.5234375  0.859375   0.31640625 0.         0.23828125 0.859375
 0.859375   0.         0.         0.53125    0.859375   0.8203125
 0.         0.859375   0.         0.859375   0.         0.33203125
 0.859375   0.         0.         0.859375   0.6953125  0.
 0.859375   0.75       0.17578125 0.140625   0.         0.859375
 0.4609375  0.2578125  0.1796875  0.859375   0.71484375 0.
 0.6875     0.         0.859375   0.4296875  0.         0.859375
 0.         0.859375   0.19140625 0.         0.859375   0.03125
 0.859375   0.         0.859375   0.2734375  0.6171875  0.
 0.         0.859375   0.859375   0.         0.         0.
 0.859375   0.33984375 0.859375   0.         0.40625    0.859375
 0.         0.         0.859375   0.         0.81640625 0.859375
 0.234375   0.         0.859375   0.859375   0.5234375  0.
 0.859375   0.859375   0.5        0.859375   0.6875     0.
 0.859375   0.7109375  0.         0.         0.859375   0.046875
 0.         0.81640625 0.         0.         0.80078125 0.0703125
 0.859375   0.16015625 0.64453125 0.         0.80078125 0.
 0.66796875 0.08984375 0.859375   0.         0.41015625 0.
 0.859375   0.859375   0.         0.         0.7890625  0.
 0.         0.859375   0.37890625 0.         0.1484375  0.859375
 0.52734375 0.         0.4765625  0.41796875 0.859375   0.
 0.6015625  0.         0.58203125 0.         0.859375   0.
 0.         0.859375   0.66796875 0.         0.         0.859375
 0.0625     0.         0.859375   0.41796875 0.         0.859375
 0.04296875 0.859375   0.8203125  0.         0.1953125  0.859375
 0.796875   0.         0.         0.74609375 0.         0.2734375
 0.859375   0.         0.1796875  0.5703125  0.         0.6875
 0.         0.859375   0.21484375 0.859375   0.859375   0.30859375
 0.         0.859375   0.30078125 0.859375   0.859375   0.
 0.859375   0.58984375 0.         0.859375   0.3828125  0.859375
 0.         0.859375   0.859375   0.         0.859375   0.
 0.         0.5859375  0.859375   0.296875   0.0546875  0.5546875
 0.         0.859375  ], y = [0. 0. 0. 0. 1.]
Sample 2: X = [0.79133858 0.82677165 0.83070866 0.79527559 0.78740157 0.82677165
 0.86220472 0.79527559 0.88188976 0.90944882 1.         1.
 0.9015748  0.85826772 0.82283465 0.83070866 0.84645669 0.7519685
 0.85433071 0.83858268 0.88188976 0.91732283 0.78740157 0.87795276
 0.77952756 0.7992126  0.78740157 0.76771654 0.71259843 0.80708661
 0.78346457 0.70866142 0.71259843 0.63385827 0.75590551 0.76377953
 0.68503937 0.80314961 0.80314961 0.7992126  0.77559055 0.71259843
 0.77952756 0.75984252 0.75984252 0.69685039 0.73622047 0.67322835
 0.68897638 0.72047244 0.64173228 0.72440945 0.61811024 0.6496063
 0.66929134 0.70866142 0.7480315  0.70866142 0.63779528 0.70866142
 0.69685039 0.6023622  0.67716535 0.52755906 0.62204724 0.66141732
 0.71259843 0.70472441 0.58267717 0.62992126 0.57480315 0.64173228
 0.68897638 0.69291339 0.62992126 0.67322835 0.65354331 0.60629921
 0.61417323 0.61811024 0.56299213 0.59055118 0.57480315 0.53937008
 0.54724409 0.53937008 0.5511811  0.62992126 0.64566929 0.60629921
 0.62598425 0.61417323 0.57086614 0.63385827 0.64566929 0.65748031
 0.57874016 0.5984252  0.5984252  0.63779528 0.63385827 0.54724409
 0.58661417 0.57874016 0.53937008 0.56692913 0.56692913 0.55905512
 0.56692913 0.51968504 0.58661417 0.62992126 0.5511811  0.5511811
 0.54330709 0.51574803 0.53149606 0.53543307 0.54724409 0.54330709
 0.56692913 0.51968504 0.56299213 0.56299213 0.43700787 0.41732283
 0.52755906 0.54330709 0.48818898 0.46062992 0.45275591 0.50787402
 0.51574803 0.53937008 0.47637795 0.46062992 0.43700787 0.42125984
 0.45669291 0.47637795 0.48425197 0.48031496 0.51574803 0.47637795
 0.48818898 0.37795276 0.4488189  0.48818898 0.49212598 0.46062992
 0.45669291 0.40944882 0.43700787 0.43700787 0.42519685 0.40944882
 0.40551181 0.46456693 0.4488189  0.4488189  0.47637795 0.4488189
 0.3503937  0.40944882 0.43307087 0.37795276 0.36614173 0.37795276
 0.42519685 0.43700787 0.3976378  0.48031496 0.49606299 0.4015748
 0.44488189 0.43307087 0.40944882 0.41338583 0.46062992 0.46850394
 0.41732283 0.44094488 0.42125984 0.42125984 0.37795276 0.40551181
 0.44094488 0.35826772 0.37795276 0.40551181 0.38188976 0.41338583
 0.42519685 0.42519685 0.42519685 0.43307087 0.36220472 0.36614173
 0.4015748  0.4015748  0.38976378 0.3976378  0.40944882 0.42913386
 0.48031496 0.48031496 0.46850394 0.37007874 0.38188976 0.36220472
 0.40551181 0.33858268 0.39370079 0.38582677 0.36220472 0.3976378
 0.4015748  0.39370079 0.38188976 0.35826772 0.36220472 0.40944882
 0.37795276 0.4015748  0.42125984 0.39370079 0.37007874 0.38188976
 0.37007874 0.37007874 0.4015748  0.3976378  0.36614173 0.41338583
 0.43307087 0.43307087 0.37401575 0.38582677 0.3976378  0.3503937
 0.37795276 0.36220472 0.38188976 0.40944882 0.42125984 0.3976378
 0.37007874 0.37795276 0.35433071 0.37795276 0.38582677 0.37401575
 0.35826772 0.36614173 0.41338583 0.46850394 0.45669291 0.
 0.86614173 0.74015748 0.12598425 0.         0.86614173 0.
 0.86614173 0.         0.04724409 0.86614173 0.         0.
 0.21259843 0.86614173 0.         0.         0.80708661 0.86614173
 0.         0.01181102 0.86614173 0.86614173 0.         0.
 0.86614173 0.         0.27952756 0.37795276 0.86614173 0.86614173
 0.         0.05905512 0.         0.73228346 0.86614173 0.86614173
 0.41338583 0.         0.78346457 0.         0.86614173 0.22047244
 0.         0.86614173 0.2519685  0.86614173 0.         0.86614173
 0.78740157 0.         0.68897638 0.26377953 0.86614173 0.09055118
 0.86614173 0.         0.13779528 0.86614173 0.         0.
 0.7007874  0.         0.         0.86614173 0.         0.86614173
 0.         0.05905512 0.86614173 0.         0.72047244 0.
 0.86614173 0.27952756 0.86614173 0.86614173 0.29133858 0.
 0.86614173 0.86614173 0.75984252 0.         0.53149606 0.86614173
 0.02362205 0.86614173 0.3503937  0.         0.86614173 0.76377953
 0.86614173 0.         0.         0.86614173 0.00787402 0.48425197
 0.         0.86614173 0.45669291 0.86614173 0.1023622  0.86614173
 0.86614173 0.48818898 0.         0.         0.86614173 0.86614173
 0.         0.42125984 0.         0.86614173 0.         0.08661417
 0.85826772 0.56692913 0.         0.         0.14566929 0.86614173
 0.86614173 0.         0.86614173 0.68897638 0.21653543 0.86614173
 0.62204724 0.         0.         0.         0.66929134 0.86614173
 0.23622047 0.13779528 0.53937008 0.         0.         0.86614173
 0.         0.86614173 0.10629921 0.86614173 0.         0.86614173
 0.52362205 0.         0.86614173 0.75590551 0.05511811 0.
 0.         0.86614173 0.32677165 0.         0.         0.83070866
 0.         0.59055118 0.         0.85433071 0.         0.75590551
 0.86614173 0.22047244 0.39370079 0.86614173 0.3976378  0.86614173
 0.06299213 0.         0.5984252  0.22440945 0.86614173 0.
 0.83464567 0.         0.86614173 0.         0.86614173 0.50787402
 0.         0.86614173 0.         0.         0.86614173 0.
 0.15354331 0.86614173 0.40944882 0.         0.86614173 0.32677165
 0.         0.4488189  0.86614173 0.86614173 0.         0.
 0.48031496 0.86614173 0.86614173 0.         0.86614173 0.48818898
 0.         0.86614173 0.86614173 0.71259843 0.         0.40944882
 0.         0.27559055 0.86614173 0.86614173 0.         0.79527559
 0.         0.86614173 0.86614173 0.19291339 0.86614173 0.
 0.59055118 0.         0.36220472 0.86614173 0.86614173 0.
 0.         0.86614173 0.86614173 0.         0.49212598 0.86614173
 0.11023622 0.         0.86614173 0.         0.86614173 0.08661417
 0.6023622  0.86614173 0.         0.86614173 0.         0.85826772
 0.16929134 0.86614173 0.         0.11811024 0.86614173 0.
 0.16929134 0.86614173], y = [0. 0. 0. 1. 0.]
Sample 3: X = [0.79133858 0.82283465 0.83070866 0.79527559 0.78346457 0.83070866
 0.86220472 0.80708661 0.88188976 0.91338583 1.         1.
 0.9015748  0.8503937  0.81102362 0.83070866 0.85826772 0.79527559
 0.8503937  0.8503937  0.88188976 0.92125984 0.80314961 0.87795276
 0.77952756 0.79133858 0.74015748 0.7519685  0.68110236 0.80314961
 0.7480315  0.69291339 0.76377953 0.68503937 0.72047244 0.78740157
 0.68503937 0.82677165 0.81102362 0.75590551 0.7519685  0.74015748
 0.77165354 0.76771654 0.7480315  0.66535433 0.70866142 0.69291339
 0.71259843 0.74015748 0.69291339 0.73228346 0.66141732 0.72047244
 0.68110236 0.7007874  0.74409449 0.70866142 0.63779528 0.70866142
 0.66929134 0.63779528 0.68110236 0.6023622  0.66141732 0.68503937
 0.69685039 0.69291339 0.54330709 0.65354331 0.61811024 0.55511811
 0.6496063  0.65748031 0.62598425 0.69685039 0.65354331 0.65354331
 0.63385827 0.62598425 0.59448819 0.62992126 0.57480315 0.59448819
 0.64173228 0.55511811 0.53543307 0.65748031 0.66141732 0.51181102
 0.65354331 0.64173228 0.55905512 0.63779528 0.59448819 0.6496063
 0.58661417 0.56299213 0.53543307 0.5984252  0.5984252  0.49212598
 0.61811024 0.57874016 0.58267717 0.54330709 0.53149606 0.58267717
 0.48031496 0.53937008 0.55511811 0.61023622 0.53543307 0.51574803
 0.55511811 0.53149606 0.50393701 0.55511811 0.58661417 0.5984252
 0.5984252  0.42913386 0.53149606 0.52362205 0.50787402 0.46850394
 0.52362205 0.53149606 0.48818898 0.46456693 0.49212598 0.5
 0.48818898 0.49606299 0.47637795 0.46850394 0.49212598 0.43307087
 0.48818898 0.48425197 0.49606299 0.46850394 0.50393701 0.46850394
 0.44094488 0.41338583 0.44094488 0.47637795 0.5        0.42913386
 0.4488189  0.36220472 0.38976378 0.44488189 0.48031496 0.45275591
 0.48031496 0.38188976 0.42913386 0.38188976 0.40551181 0.42913386
 0.38582677 0.34645669 0.4488189  0.47637795 0.44488189 0.4015748
 0.43700787 0.38976378 0.4015748  0.49212598 0.46850394 0.41338583
 0.4015748  0.42913386 0.42913386 0.42913386 0.4488189  0.4015748
 0.42125984 0.44488189 0.40551181 0.39370079 0.43307087 0.42125984
 0.36220472 0.4015748  0.38582677 0.37007874 0.40944882 0.39370079
 0.37401575 0.40551181 0.42125984 0.40944882 0.37795276 0.3976378
 0.3976378  0.42913386 0.35433071 0.44094488 0.46062992 0.42913386
 0.47637795 0.48031496 0.46456693 0.41338583 0.36220472 0.43700787
 0.38188976 0.39370079 0.37007874 0.38582677 0.3503937  0.3976378
 0.38976378 0.36614173 0.3503937  0.36220472 0.3503937  0.36220472
 0.37795276 0.37007874 0.36220472 0.36220472 0.34251969 0.40551181
 0.43307087 0.38976378 0.38582677 0.37795276 0.36614173 0.37007874
 0.40944882 0.38582677 0.37007874 0.38188976 0.39370079 0.39370079
 0.3976378  0.38582677 0.38582677 0.40944882 0.42125984 0.40551181
 0.3503937  0.38188976 0.40944882 0.40944882 0.41732283 0.38188976
 0.4015748  0.37795276 0.35826772 0.45669291 0.45669291 0.
 0.86614173 0.74409449 0.14566929 0.         0.86614173 0.
 0.86614173 0.         0.05511811 0.86614173 0.         0.
 0.14173228 0.86614173 0.         0.         0.84645669 0.86614173
 0.         0.01574803 0.86614173 0.86614173 0.         0.
 0.86614173 0.         0.12992126 0.38582677 0.86614173 0.86614173
 0.         0.         0.03543307 0.69291339 0.86614173 0.
 0.47244094 0.         0.75984252 0.         0.86614173 0.38976378
 0.         0.86614173 0.22834646 0.86614173 0.         0.86614173
 0.         0.         0.86614173 0.62204724 0.         0.37401575
 0.86614173 0.         0.         0.86614173 0.         0.
 0.76377953 0.68897638 0.         0.86614173 0.         0.86614173
 0.         0.         0.86614173 0.86614173 0.68503937 0.
 0.         0.43307087 0.86614173 0.86614173 0.42125984 0.
 0.69685039 0.         0.86614173 0.         0.6023622  0.86614173
 0.         0.86614173 0.33070866 0.         0.         0.83858268
 0.59055118 0.         0.         0.86614173 0.01968504 0.83464567
 0.         0.         0.86614173 0.86614173 0.2480315  0.
 0.86614173 0.73228346 0.         0.05905512 0.86614173 0.86614173
 0.         0.63385827 0.         0.86614173 0.         0.78740157
 0.75590551 0.54724409 0.         0.86614173 0.39370079 0.86614173
 0.86614173 0.         0.86614173 0.2007874  0.03937008 0.86614173
 0.60629921 0.86614173 0.21259843 0.         0.86614173 0.86614173
 0.37795276 0.         0.83070866 0.         0.         0.39370079
 0.86614173 0.86614173 0.02755906 0.86614173 0.         0.
 0.86614173 0.         0.86614173 0.         0.5        0.
 0.73228346 0.86614173 0.         0.         0.83858268 0.86614173
 0.         0.         0.         0.86614173 0.         0.64173228
 0.86614173 0.         0.86614173 0.86614173 0.         0.81889764
 0.66929134 0.         0.31496063 0.56299213 0.86614173 0.
 0.57874016 0.         0.86614173 0.         0.         0.86614173
 0.         0.70472441 0.         0.51181102 0.86614173 0.86614173
 0.20472441 0.86614173 0.         0.86614173 0.86614173 0.66535433
 0.         0.81102362 0.         0.86614173 0.86614173 0.22440945
 0.64566929 0.         0.86614173 0.         0.86614173 0.
 0.         0.86614173 0.86614173 0.         0.86614173 0.86614173
 0.         0.51968504 0.86614173 0.         0.86614173 0.11023622
 0.7480315  0.86614173 0.         0.28346457 0.86614173 0.86614173
 0.45275591 0.         0.86614173 0.         0.86614173 0.
 0.         0.8503937  0.86614173 0.         0.31496063 0.86614173
 0.03543307 0.         0.54330709 0.86614173 0.         0.86614173
 0.         0.86614173 0.04724409 0.86614173 0.         0.86614173
 0.04330709 0.86614173 0.         0.36220472 0.         0.42913386
 0.         0.86614173], y = [0. 0. 0. 1. 0.]
Sample 4: X = [0.76953125 0.8125     0.8203125  0.79296875 0.7578125  0.8125
 0.84765625 0.83203125 0.859375   0.8984375  0.99609375 1.
 0.87109375 0.8203125  0.78515625 0.80078125 0.76171875 0.8515625
 0.84375    0.68359375 0.8828125  0.87890625 0.78515625 0.83984375
 0.74609375 0.76953125 0.74609375 0.80859375 0.77734375 0.65234375
 0.72265625 0.79296875 0.765625   0.71875    0.7265625  0.73046875
 0.80078125 0.68359375 0.81640625 0.80859375 0.65625    0.796875
 0.75390625 0.6796875  0.69921875 0.78515625 0.8125     0.7734375
 0.80078125 0.7734375  0.7734375  0.73828125 0.8046875  0.66015625
 0.75390625 0.75390625 0.73828125 0.74609375 0.74609375 0.734375
 0.76953125 0.7578125  0.73828125 0.78125    0.76171875 0.7734375
 0.78515625 0.7109375  0.765625   0.74609375 0.72265625 0.7265625
 0.63671875 0.66796875 0.734375   0.69140625 0.703125   0.71484375
 0.66796875 0.70703125 0.75       0.73046875 0.61328125 0.625
 0.68359375 0.69140625 0.609375   0.67578125 0.56640625 0.6640625
 0.68359375 0.7265625  0.6640625  0.67578125 0.703125   0.75
 0.703125   0.62109375 0.64453125 0.66015625 0.67578125 0.62109375
 0.69140625 0.66015625 0.55859375 0.625      0.59375    0.6796875
 0.703125   0.671875   0.66015625 0.671875   0.703125   0.6328125
 0.66796875 0.65234375 0.63671875 0.54296875 0.6875     0.625
 0.59765625 0.46875    0.6171875  0.63671875 0.57421875 0.61328125
 0.6171875  0.6015625  0.51171875 0.59765625 0.60546875 0.62890625
 0.6015625  0.640625   0.59765625 0.62109375 0.609375   0.5703125
 0.61328125 0.55859375 0.53125    0.57421875 0.59765625 0.515625
 0.48828125 0.62109375 0.6171875  0.57421875 0.5        0.578125
 0.59765625 0.56640625 0.578125   0.62890625 0.58984375 0.56640625
 0.5703125  0.58203125 0.58203125 0.54296875 0.5546875  0.51171875
 0.5        0.58984375 0.59765625 0.5390625  0.4453125  0.53125
 0.51171875 0.54296875 0.55859375 0.484375   0.54296875 0.51171875
 0.4765625  0.49609375 0.578125   0.57421875 0.54296875 0.5546875
 0.46875    0.515625   0.5390625  0.5625     0.47265625 0.46875
 0.50390625 0.4921875  0.53125    0.55859375 0.4296875  0.453125
 0.52734375 0.546875   0.5078125  0.5234375  0.52734375 0.5078125
 0.48828125 0.52734375 0.43359375 0.48046875 0.52734375 0.53515625
 0.49609375 0.4921875  0.53515625 0.5234375  0.4765625  0.53515625
 0.47265625 0.3984375  0.51171875 0.46875    0.46875    0.44140625
 0.42578125 0.46875    0.453125   0.4765625  0.4765625  0.46875
 0.484375   0.41015625 0.44140625 0.41015625 0.44921875 0.453125
 0.3828125  0.45703125 0.44140625 0.5        0.49609375 0.390625
 0.3984375  0.45703125 0.4609375  0.41015625 0.44140625 0.4375
 0.453125   0.453125   0.4609375  0.484375   0.39453125 0.45703125
 0.34375    0.4296875  0.421875   0.45703125 0.4609375  0.3828125
 0.4453125  0.40625    0.421875   0.4609375  0.47265625 0.
 0.859375   0.6015625  0.         0.         0.859375   0.50390625
 0.859375   0.         0.05078125 0.859375   0.         0.
 0.5        0.         0.         0.859375   0.2734375  0.859375
 0.859375   0.171875   0.75       0.859375   0.296875   0.80859375
 0.         0.03125    0.859375   0.421875   0.546875   0.859375
 0.3046875  0.859375   0.109375   0.         0.859375   0.859375
 0.         0.859375   0.         0.3125     0.859375   0.
 0.859375   0.18359375 0.859375   0.2578125  0.859375   0.
 0.         0.859375   0.359375   0.         0.859375   0.
 0.84375    0.73828125 0.         0.         0.859375   0.
 0.52734375 0.859375   0.1953125  0.859375   0.171875   0.859375
 0.859375   0.         0.859375   0.44140625 0.859375   0.76171875
 0.         0.859375   0.         0.859375   0.         0.1875
 0.859375   0.         0.         0.859375   0.63671875 0.
 0.859375   0.5546875  0.         0.23046875 0.         0.859375
 0.70703125 0.0234375  0.0703125  0.859375   0.5390625  0.
 0.3828125  0.         0.859375   0.3203125  0.         0.859375
 0.         0.85546875 0.2734375  0.         0.859375   0.
 0.6796875  0.         0.859375   0.05859375 0.53515625 0.
 0.         0.859375   0.83984375 0.         0.         0.
 0.859375   0.14453125 0.859375   0.859375   0.15625    0.859375
 0.         0.         0.859375   0.         0.76953125 0.859375
 0.         0.1640625  0.859375   0.859375   0.36328125 0.
 0.76171875 0.         0.609375   0.859375   0.         0.
 0.859375   0.671875   0.3671875  0.         0.859375   0.4453125
 0.         0.44140625 0.859375   0.         0.66015625 0.
 0.859375   0.         0.59765625 0.         0.83203125 0.
 0.59375    0.         0.859375   0.859375   0.5546875  0.
 0.859375   0.52734375 0.         0.859375   0.609375   0.859375
 0.         0.859375   0.359375   0.         0.2890625  0.859375
 0.265625   0.859375   0.17578125 0.6953125  0.         0.3125
 0.75       0.         0.71484375 0.6953125  0.4296875  0.
 0.296875   0.859375   0.31640625 0.         0.         0.859375
 0.125      0.         0.859375   0.51171875 0.         0.859375
 0.         0.859375   0.35546875 0.         0.5390625  0.859375
 0.71484375 0.         0.         0.         0.859375   0.40234375
 0.859375   0.859375   0.         0.375      0.859375   0.5
 0.859375   0.17578125 0.47265625 0.859375   0.         0.515625
 0.         0.859375   0.46875    0.859375   0.859375   0.21875
 0.         0.4296875  0.859375   0.         0.33984375 0.
 0.78515625 0.859375   0.         0.         0.859375   0.859375
 0.         0.859375   0.         0.859375   0.57421875 0.0078125
 0.         0.859375  ], y = [0. 0. 0. 0. 1.]
Sample 5: X = [0.7578125  0.8125     0.8203125  0.79296875 0.75       0.8203125
 0.8515625  0.83984375 0.87109375 0.89453125 0.99609375 1.
 0.8671875  0.796875   0.7890625  0.82421875 0.671875   0.8515625
 0.84765625 0.7890625  0.875      0.86328125 0.73828125 0.84765625
 0.796875   0.71875    0.7734375  0.80859375 0.78515625 0.6640625
 0.69921875 0.80859375 0.81640625 0.73828125 0.68359375 0.71484375
 0.7890625  0.6484375  0.79296875 0.7265625  0.7734375  0.80078125
 0.76953125 0.4609375  0.70703125 0.76953125 0.78515625 0.734375
 0.78515625 0.8046875  0.76953125 0.69921875 0.796875   0.73046875
 0.71875    0.78515625 0.74609375 0.80078125 0.76953125 0.7421875
 0.75390625 0.7421875  0.74609375 0.6953125  0.765625   0.80078125
 0.73046875 0.70703125 0.78125    0.796875   0.72265625 0.7109375
 0.69140625 0.6640625  0.69921875 0.70703125 0.671875   0.69921875
 0.62890625 0.6875     0.71484375 0.70703125 0.7109375  0.72265625
 0.65234375 0.71875    0.69921875 0.66796875 0.67578125 0.7109375
 0.72265625 0.7109375  0.609375   0.65234375 0.66796875 0.68359375
 0.57421875 0.6484375  0.66796875 0.6875     0.7109375  0.67578125
 0.63671875 0.66796875 0.64453125 0.6640625  0.60546875 0.59375
 0.6484375  0.65234375 0.6015625  0.64453125 0.6953125  0.66796875
 0.6171875  0.6171875  0.6640625  0.58984375 0.6484375  0.65234375
 0.59765625 0.59375    0.65234375 0.6640625  0.54296875 0.5546875
 0.5859375  0.53125    0.63671875 0.64453125 0.58984375 0.5546875
 0.54296875 0.57421875 0.55078125 0.5546875  0.625      0.625
 0.578125   0.5625     0.6015625  0.62890625 0.5703125  0.54296875
 0.6015625  0.5859375  0.58984375 0.61328125 0.5859375  0.60546875
 0.48046875 0.57421875 0.57421875 0.55859375 0.5546875  0.5859375
 0.54296875 0.44921875 0.53515625 0.4921875  0.546875   0.48828125
 0.5625     0.57421875 0.5859375  0.54296875 0.55078125 0.53515625
 0.4921875  0.46875    0.578125   0.54296875 0.55859375 0.5
 0.4375     0.5625     0.5078125  0.54296875 0.53515625 0.3984375
 0.41796875 0.4609375  0.5078125  0.48828125 0.46484375 0.484375
 0.43359375 0.49609375 0.50390625 0.42578125 0.41015625 0.5078125
 0.44140625 0.46484375 0.49609375 0.51953125 0.55859375 0.49609375
 0.5390625  0.47265625 0.546875   0.4296875  0.4765625  0.45703125
 0.515625   0.484375   0.48828125 0.4140625  0.5        0.4140625
 0.48828125 0.42578125 0.46875    0.44921875 0.4765625  0.49609375
 0.44921875 0.43359375 0.42578125 0.4453125  0.453125   0.48828125
 0.47265625 0.46875    0.47265625 0.45703125 0.40234375 0.41015625
 0.46875    0.3671875  0.43359375 0.4453125  0.48046875 0.4375
 0.44140625 0.453125   0.39453125 0.421875   0.4140625  0.42578125
 0.375      0.47265625 0.48046875 0.3984375  0.41796875 0.4375
 0.44140625 0.46484375 0.40625    0.421875   0.44921875 0.44140625
 0.44140625 0.45703125 0.453125   0.49609375 0.48828125 0.
 0.859375   0.609375   0.         0.         0.859375   0.4921875
 0.859375   0.         0.046875   0.859375   0.         0.
 0.39453125 0.         0.         0.859375   0.15625    0.859375
 0.         0.21484375 0.48828125 0.859375   0.046875   0.80078125
 0.859375   0.         0.859375   0.3515625  0.         0.859375
 0.28515625 0.859375   0.         0.         0.859375   0.859375
 0.         0.859375   0.859375   0.2890625  0.859375   0.
 0.60546875 0.         0.859375   0.578125   0.62109375 0.
 0.859375   0.859375   0.         0.859375   0.859375   0.
 0.859375   0.4609375  0.         0.859375   0.74609375 0.
 0.4765625  0.859375   0.         0.859375   0.1484375  0.
 0.546875   0.         0.8359375  0.         0.61328125 0.1796875
 0.         0.859375   0.         0.8515625  0.         0.
 0.859375   0.         0.140625   0.859375   0.1171875  0.
 0.56640625 0.28125    0.859375   0.         0.859375   0.67578125
 0.         0.         0.         0.859375   0.         0.25
 0.859375   0.         0.859375   0.         0.859375   0.81640625
 0.         0.859375   0.34375    0.859375   0.         0.859375
 0.05078125 0.         0.859375   0.         0.         0.859375
 0.859375   0.         0.578125   0.         0.859375   0.859375
 0.625      0.         0.         0.859375   0.859375   0.
 0.38671875 0.859375   0.         0.375      0.51953125 0.859375
 0.859375   0.859375   0.23828125 0.859375   0.1328125  0.859375
 0.23046875 0.859375   0.50390625 0.171875   0.         0.859375
 0.75       0.         0.859375   0.10546875 0.859375   0.859375
 0.015625   0.         0.859375   0.671875   0.         0.66015625
 0.859375   0.32421875 0.         0.6640625  0.6640625  0.859375
 0.         0.859375   0.859375   0.02734375 0.         0.
 0.859375   0.         0.37109375 0.859375   0.859375   0.
 0.859375   0.70703125 0.         0.1953125  0.640625   0.859375
 0.         0.859375   0.44140625 0.859375   0.         0.859375
 0.         0.         0.859375   0.859375   0.         0.12109375
 0.         0.59375    0.859375   0.5703125  0.         0.5625
 0.859375   0.48046875 0.33984375 0.078125   0.859375   0.
 0.59765625 0.21484375 0.         0.859375   0.859375   0.
 0.         0.859375   0.0078125  0.859375   0.         0.
 0.859375   0.453125   0.         0.5234375  0.859375   0.
 0.75390625 0.         0.4296875  0.53125    0.859375   0.859375
 0.30078125 0.578125   0.         0.859375   0.1015625  0.859375
 0.         0.00390625 0.859375   0.29296875 0.         0.859375
 0.55859375 0.         0.         0.69140625 0.         0.859375
 0.05859375 0.859375   0.         0.3125     0.859375   0.
 0.15625    0.859375  ], y = [1. 0. 0. 0. 0.]
Sample 1: X = [0.77559055 0.82283465 0.82677165 0.80708661 0.79527559 0.81889764
 0.84645669 0.88582677 0.92125984 0.90944882 1.         0.99212598
 0.90551181 0.80314961 0.77952756 0.7992126  0.8503937  0.6496063
 0.86220472 0.77559055 0.87401575 0.82283465 0.73228346 0.82677165
 0.84251969 0.82283465 0.78740157 0.7992126  0.71259843 0.78346457
 0.83070866 0.84251969 0.66535433 0.70866142 0.77165354 0.65748031
 0.79133858 0.74015748 0.80708661 0.7519685  0.78740157 0.80314961
 0.7519685  0.76771654 0.7519685  0.7480315  0.72047244 0.7992126
 0.78346457 0.7480315  0.74015748 0.76377953 0.73622047 0.71653543
 0.67716535 0.66929134 0.72834646 0.76771654 0.6496063  0.63779528
 0.74409449 0.71653543 0.73622047 0.67716535 0.71653543 0.69291339
 0.75984252 0.69291339 0.68110236 0.71653543 0.69291339 0.58661417
 0.61417323 0.66929134 0.72440945 0.74409449 0.60629921 0.72047244
 0.63385827 0.70472441 0.70866142 0.70866142 0.51574803 0.55905512
 0.69685039 0.70866142 0.64173228 0.67716535 0.52755906 0.68897638
 0.62992126 0.62992126 0.69685039 0.64566929 0.70866142 0.6023622
 0.68503937 0.66535433 0.5984252  0.65748031 0.62992126 0.64566929
 0.64566929 0.54330709 0.66535433 0.59055118 0.64566929 0.57874016
 0.57874016 0.53149606 0.56299213 0.53149606 0.58267717 0.58661417
 0.53937008 0.57086614 0.49606299 0.6023622  0.63779528 0.63385827
 0.64173228 0.61417323 0.56299213 0.54330709 0.58267717 0.51181102
 0.54724409 0.62598425 0.62992126 0.61811024 0.60629921 0.57480315
 0.59448819 0.61023622 0.4488189  0.60629921 0.57480315 0.5984252
 0.50393701 0.58661417 0.50787402 0.56692913 0.5511811  0.61811024
 0.5511811  0.53149606 0.5511811  0.54724409 0.52362205 0.6023622
 0.57480315 0.50393701 0.48425197 0.53149606 0.52755906 0.49212598
 0.5        0.54724409 0.53937008 0.51968504 0.48818898 0.37795276
 0.43307087 0.44094488 0.50787402 0.53543307 0.51968504 0.49606299
 0.47637795 0.47637795 0.45669291 0.50787402 0.43307087 0.48031496
 0.48425197 0.48818898 0.47244094 0.48031496 0.51574803 0.49212598
 0.4488189  0.46456693 0.40944882 0.51181102 0.49606299 0.51574803
 0.47637795 0.47244094 0.52755906 0.44488189 0.40944882 0.47244094
 0.47637795 0.5        0.50787402 0.5        0.44488189 0.5
 0.43700787 0.47244094 0.44094488 0.48818898 0.46850394 0.45275591
 0.46456693 0.43700787 0.42913386 0.45275591 0.43307087 0.40944882
 0.43700787 0.46062992 0.43307087 0.37007874 0.38582677 0.4488189
 0.44488189 0.42519685 0.48818898 0.42125984 0.44094488 0.44094488
 0.42125984 0.39370079 0.4015748  0.48031496 0.46456693 0.45669291
 0.42519685 0.38582677 0.42913386 0.41338583 0.34645669 0.40551181
 0.43307087 0.42519685 0.3976378  0.37401575 0.40944882 0.42125984
 0.42125984 0.44094488 0.3976378  0.33858268 0.43700787 0.38976378
 0.40551181 0.4488189  0.40944882 0.3976378  0.40944882 0.39370079
 0.3503937  0.38188976 0.41338583 0.46456693 0.46850394 0.
 0.86614173 0.67322835 0.         0.         0.86614173 0.2007874
 0.86614173 0.         0.07874016 0.86614173 0.         0.5
 0.         0.86614173 0.         0.30314961 0.86614173 0.86614173
 0.         0.27952756 0.         0.86614173 0.75984252 0.
 0.86614173 0.         0.86614173 0.86614173 0.86614173 0.
 0.         0.86614173 0.         0.60629921 0.82677165 0.
 0.03543307 0.86614173 0.86614173 0.48818898 0.01574803 0.86614173
 0.         0.77952756 0.         0.86614173 0.         0.22834646
 0.86614173 0.86614173 0.         0.74015748 0.         0.30708661
 0.03149606 0.86614173 0.         0.86614173 0.12992126 0.86614173
 0.         0.7480315  0.         0.         0.86614173 0.
 0.22834646 0.86614173 0.         0.46850394 0.86614173 0.
 0.86614173 0.         0.86614173 0.86614173 0.         0.0511811
 0.         0.7007874  0.86614173 0.         0.         0.66535433
 0.86614173 0.16141732 0.86614173 0.         0.56692913 0.
 0.61811024 0.03937008 0.         0.86614173 0.58267717 0.
 0.86614173 0.30314961 0.86614173 0.         0.77952756 0.86614173
 0.44094488 0.         0.86614173 0.         0.69685039 0.74409449
 0.86614173 0.08661417 0.         0.86614173 0.86614173 0.18110236
 0.86614173 0.46850394 0.         0.54724409 0.         0.83464567
 0.         0.86614173 0.48425197 0.         0.86614173 0.
 0.86614173 0.27559055 0.         0.5511811  0.86614173 0.18503937
 0.22834646 0.86614173 0.64173228 0.         0.86614173 0.86614173
 0.         0.         0.         0.86614173 0.27165354 0.86614173
 0.         0.43307087 0.86614173 0.         0.86614173 0.22834646
 0.11417323 0.         0.86614173 0.42125984 0.26377953 0.86614173
 0.2992126  0.86614173 0.13779528 0.86614173 0.86614173 0.
 0.45669291 0.         0.75590551 0.06299213 0.         0.86614173
 0.52755906 0.         0.86614173 0.86614173 0.         0.86614173
 0.11417323 0.86614173 0.06692913 0.86614173 0.         0.86614173
 0.3503937  0.         0.86614173 0.65748031 0.34645669 0.
 0.86614173 0.03937008 0.46456693 0.86614173 0.35433071 0.0984252
 0.         0.86614173 0.50393701 0.         0.86614173 0.86614173
 0.         0.         0.86614173 0.86614173 0.1023622  0.86614173
 0.86614173 0.         0.68110236 0.86614173 0.         0.31102362
 0.86614173 0.         0.         0.65748031 0.         0.86614173
 0.11811024 0.86614173 0.         0.72834646 0.         0.
 0.         0.75590551 0.         0.         0.63779528 0.
 0.86614173 0.         0.74015748 0.86614173 0.72440945 0.
 0.         0.86614173 0.         0.86614173 0.5511811  0.
 0.86614173 0.         0.26771654 0.         0.86614173 0.86614173
 0.         0.63779528 0.29133858 0.         0.86614173 0.86614173
 0.21259843 0.86614173], y = [0. 0. 1. 0. 0.]
Sample 2: X = [0.77559055 0.82283465 0.82677165 0.80708661 0.79527559 0.81889764
 0.84645669 0.88582677 0.92125984 0.90944882 1.         0.99212598
 0.90551181 0.80314961 0.78346457 0.7992126  0.8503937  0.6496063
 0.86220472 0.77559055 0.87401575 0.82283465 0.73622047 0.82677165
 0.84251969 0.82283465 0.78346457 0.7992126  0.71653543 0.78346457
 0.83070866 0.84251969 0.66929134 0.70866142 0.76771654 0.65354331
 0.79133858 0.74409449 0.80708661 0.7480315  0.79133858 0.80314961
 0.7519685  0.76771654 0.7519685  0.7480315  0.72047244 0.7992126
 0.78346457 0.7519685  0.74015748 0.76377953 0.73622047 0.72047244
 0.68110236 0.66535433 0.72834646 0.76771654 0.64566929 0.63779528
 0.74409449 0.72047244 0.73622047 0.67716535 0.71653543 0.69291339
 0.75984252 0.69291339 0.68110236 0.71653543 0.68897638 0.58661417
 0.61417323 0.66535433 0.72047244 0.74015748 0.5984252  0.72047244
 0.63385827 0.70472441 0.70866142 0.70866142 0.5        0.55511811
 0.7007874  0.70866142 0.64566929 0.68110236 0.52755906 0.68503937
 0.62598425 0.62992126 0.69685039 0.64566929 0.70866142 0.6023622
 0.68503937 0.66535433 0.5984252  0.65748031 0.62992126 0.6496063
 0.6496063  0.53149606 0.66535433 0.59055118 0.64566929 0.57480315
 0.57874016 0.51968504 0.55905512 0.52362205 0.58661417 0.58661417
 0.53937008 0.56692913 0.49606299 0.6023622  0.63779528 0.62992126
 0.64173228 0.61023622 0.55905512 0.53543307 0.58267717 0.51574803
 0.53937008 0.62598425 0.62992126 0.61811024 0.60629921 0.57086614
 0.5984252  0.61023622 0.44488189 0.6023622  0.57480315 0.59448819
 0.50393701 0.58661417 0.50393701 0.56299213 0.55905512 0.61811024
 0.54330709 0.53937008 0.55511811 0.54330709 0.52362205 0.6023622
 0.57874016 0.5        0.49606299 0.52755906 0.53543307 0.49212598
 0.5        0.5511811  0.53937008 0.51574803 0.48425197 0.33464567
 0.44488189 0.43307087 0.51574803 0.54330709 0.52362205 0.5
 0.47637795 0.48425197 0.46456693 0.51574803 0.4488189  0.47637795
 0.48818898 0.49212598 0.46062992 0.47637795 0.50787402 0.48425197
 0.4488189  0.46850394 0.42519685 0.50787402 0.49212598 0.51968504
 0.48425197 0.46850394 0.52755906 0.45669291 0.39370079 0.47244094
 0.48425197 0.5        0.50393701 0.49606299 0.43700787 0.49606299
 0.44488189 0.46062992 0.44488189 0.48425197 0.47244094 0.44094488
 0.48031496 0.46850394 0.4488189  0.42913386 0.39370079 0.3976378
 0.41732283 0.47637795 0.4488189  0.35433071 0.38188976 0.45275591
 0.45669291 0.42913386 0.49212598 0.43700787 0.44094488 0.41338583
 0.41338583 0.40551181 0.3976378  0.46850394 0.46456693 0.46850394
 0.42125984 0.37795276 0.4015748  0.4015748  0.34251969 0.33858268
 0.41338583 0.41338583 0.35433071 0.4015748  0.38188976 0.43307087
 0.39370079 0.42913386 0.4015748  0.38976378 0.44488189 0.3976378
 0.41338583 0.44488189 0.3976378  0.42125984 0.4015748  0.39370079
 0.36614173 0.35433071 0.42913386 0.47637795 0.46456693 0.
 0.86614173 0.67716535 0.         0.         0.86614173 0.2007874
 0.86614173 0.         0.07874016 0.86614173 0.         0.5
 0.         0.86614173 0.         0.34251969 0.86614173 0.86614173
 0.         0.28740157 0.         0.86614173 0.75984252 0.
 0.86614173 0.         0.86614173 0.86220472 0.86614173 0.
 0.         0.86614173 0.         0.53937008 0.81496063 0.
 0.03543307 0.86614173 0.86614173 0.48818898 0.02755906 0.86614173
 0.         0.77559055 0.         0.86614173 0.         0.22440945
 0.86614173 0.86614173 0.         0.74015748 0.         0.31496063
 0.03543307 0.86614173 0.         0.86614173 0.12992126 0.86614173
 0.         0.73622047 0.         0.         0.86614173 0.
 0.21653543 0.86614173 0.         0.40551181 0.86614173 0.
 0.86614173 0.         0.86614173 0.86614173 0.         0.04724409
 0.         0.70472441 0.86614173 0.         0.         0.6496063
 0.86614173 0.16141732 0.86614173 0.         0.56299213 0.
 0.61023622 0.04330709 0.         0.86614173 0.56692913 0.
 0.86614173 0.30708661 0.86614173 0.         0.78346457 0.86614173
 0.42913386 0.         0.86614173 0.         0.69685039 0.72440945
 0.86614173 0.02755906 0.         0.86614173 0.86614173 0.18503937
 0.86614173 0.42913386 0.86614173 0.5511811  0.         0.82283465
 0.         0.86614173 0.48425197 0.         0.86614173 0.
 0.86614173 0.27559055 0.         0.54330709 0.86614173 0.19685039
 0.13385827 0.86614173 0.64173228 0.         0.86614173 0.86614173
 0.         0.         0.         0.86614173 0.23228346 0.86614173
 0.         0.4015748  0.86614173 0.         0.86614173 0.2992126
 0.08267717 0.         0.86614173 0.4015748  0.19291339 0.86614173
 0.25590551 0.86614173 0.17716535 0.18897638 0.86614173 0.
 0.39370079 0.         0.7519685  0.0511811  0.         0.86614173
 0.70472441 0.         0.86614173 0.86614173 0.         0.86614173
 0.09055118 0.86614173 0.09055118 0.86614173 0.         0.86614173
 0.23228346 0.         0.86614173 0.56692913 0.35826772 0.
 0.86614173 0.19685039 0.20472441 0.86614173 0.49606299 0.09055118
 0.         0.86614173 0.49212598 0.0511811  0.86614173 0.86614173
 0.         0.         0.86614173 0.86614173 0.         0.61417323
 0.03937008 0.         0.80314961 0.86614173 0.         0.31496063
 0.86614173 0.86614173 0.         0.71653543 0.         0.86614173
 0.01574803 0.86614173 0.         0.67716535 0.         0.86614173
 0.         0.66929134 0.         0.86614173 0.5984252  0.
 0.86614173 0.         0.80314961 0.         0.72440945 0.
 0.         0.         0.         0.86614173 0.57480315 0.
 0.86614173 0.86614173 0.35433071 0.         0.86614173 0.86614173
 0.         0.72834646 0.         0.         0.86614173 0.71653543
 0.13779528 0.86614173], y = [0. 0. 1. 0. 0.]
Sample 3: X = [0.77559055 0.82283465 0.82677165 0.80708661 0.79527559 0.81889764
 0.84645669 0.88582677 0.92125984 0.91338583 1.         0.99212598
 0.90551181 0.80314961 0.78346457 0.80314961 0.85433071 0.64566929
 0.86220472 0.77559055 0.87795276 0.82283465 0.73622047 0.82677165
 0.84251969 0.82283465 0.78740157 0.7992126  0.71653543 0.78346457
 0.83070866 0.84251969 0.67716535 0.70866142 0.76771654 0.6496063
 0.79133858 0.74409449 0.80708661 0.7480315  0.79133858 0.7992126
 0.7519685  0.76771654 0.7519685  0.7480315  0.72047244 0.7992126
 0.77952756 0.7519685  0.74015748 0.76377953 0.73622047 0.72047244
 0.68110236 0.66141732 0.72834646 0.76771654 0.64566929 0.63779528
 0.74409449 0.72047244 0.73622047 0.67716535 0.71653543 0.68897638
 0.75590551 0.69291339 0.68110236 0.71653543 0.68897638 0.58267717
 0.61417323 0.66535433 0.72047244 0.74015748 0.59448819 0.72047244
 0.63385827 0.7007874  0.70866142 0.70472441 0.48818898 0.5511811
 0.7007874  0.70866142 0.64173228 0.67716535 0.52755906 0.68503937
 0.62598425 0.62992126 0.69291339 0.64566929 0.70866142 0.5984252
 0.68110236 0.66535433 0.6023622  0.65748031 0.62598425 0.6496063
 0.64566929 0.52755906 0.66535433 0.58661417 0.64173228 0.57086614
 0.57086614 0.53149606 0.55511811 0.51181102 0.58661417 0.58661417
 0.53937008 0.56692913 0.48818898 0.5984252  0.63385827 0.62992126
 0.63779528 0.60629921 0.55905512 0.53543307 0.57874016 0.51181102
 0.53543307 0.62204724 0.62598425 0.61417323 0.60629921 0.56692913
 0.59055118 0.60629921 0.46062992 0.6023622  0.57480315 0.59055118
 0.48425197 0.58661417 0.5        0.56692913 0.5511811  0.61023622
 0.53937008 0.53149606 0.54330709 0.54330709 0.51968504 0.5984252
 0.57480315 0.5        0.48031496 0.52755906 0.53149606 0.48818898
 0.49606299 0.54724409 0.53149606 0.50393701 0.48031496 0.37007874
 0.43700787 0.44094488 0.51574803 0.53543307 0.51574803 0.49212598
 0.46850394 0.48425197 0.46850394 0.50787402 0.44094488 0.48031496
 0.48031496 0.47637795 0.46062992 0.47637795 0.50787402 0.48425197
 0.45275591 0.46850394 0.43307087 0.50393701 0.48425197 0.51181102
 0.46062992 0.46850394 0.52755906 0.4488189  0.38582677 0.46850394
 0.47637795 0.5        0.49606299 0.49212598 0.43700787 0.49606299
 0.43700787 0.45275591 0.40944882 0.48031496 0.46062992 0.41338583
 0.45669291 0.46850394 0.45669291 0.43700787 0.38976378 0.42519685
 0.43307087 0.46850394 0.43307087 0.36614173 0.36614173 0.44488189
 0.44094488 0.40551181 0.48425197 0.42519685 0.43700787 0.42519685
 0.37401575 0.39370079 0.37795276 0.46456693 0.46062992 0.46456693
 0.42519685 0.3976378  0.41732283 0.4015748  0.36220472 0.39370079
 0.42519685 0.37401575 0.36614173 0.37795276 0.38188976 0.42519685
 0.4015748  0.40944882 0.40944882 0.37401575 0.43700787 0.37007874
 0.3976378  0.4488189  0.39370079 0.40944882 0.37401575 0.38976378
 0.34251969 0.31102362 0.41732283 0.44488189 0.47244094 0.
 0.86614173 0.67716535 0.         0.         0.86614173 0.20472441
 0.86614173 0.         0.08267717 0.86614173 0.         0.50393701
 0.         0.86614173 0.         0.29133858 0.86614173 0.86614173
 0.         0.29527559 0.         0.86614173 0.76377953 0.
 0.86614173 0.         0.86614173 0.86614173 0.86614173 0.
 0.         0.86614173 0.         0.55511811 0.82677165 0.
 0.04330709 0.86614173 0.86614173 0.50393701 0.04724409 0.86614173
 0.         0.79133858 0.         0.86614173 0.         0.24409449
 0.86614173 0.86614173 0.         0.7480315  0.         0.33464567
 0.05511811 0.86614173 0.         0.86614173 0.14566929 0.86614173
 0.         0.7480315  0.         0.         0.86614173 0.
 0.24409449 0.86614173 0.         0.43307087 0.86614173 0.
 0.86614173 0.         0.86614173 0.86614173 0.         0.07086614
 0.         0.73228346 0.86614173 0.         0.         0.67716535
 0.86614173 0.17716535 0.86614173 0.         0.5984252  0.
 0.63779528 0.06692913 0.         0.86614173 0.59448819 0.
 0.86614173 0.33858268 0.86614173 0.         0.81889764 0.86614173
 0.4488189  0.         0.86614173 0.         0.7480315  0.68503937
 0.86614173 0.11811024 0.         0.86614173 0.86614173 0.19685039
 0.86614173 0.48818898 0.         0.57480315 0.         0.86614173
 0.         0.86614173 0.52755906 0.         0.86614173 0.
 0.86614173 0.31496063 0.         0.58267717 0.86614173 0.23622047
 0.15748031 0.86614173 0.68503937 0.         0.86614173 0.86614173
 0.         0.         0.         0.86614173 0.27559055 0.86614173
 0.         0.43307087 0.86614173 0.         0.86614173 0.3976378
 0.         0.         0.86614173 0.40551181 0.26377953 0.86614173
 0.33070866 0.86614173 0.19291339 0.85433071 0.86614173 0.
 0.36220472 0.86614173 0.7992126  0.04724409 0.         0.86614173
 0.52362205 0.         0.86614173 0.86614173 0.         0.86614173
 0.08661417 0.86614173 0.14566929 0.86614173 0.         0.86614173
 0.15748031 0.         0.86614173 0.68110236 0.24015748 0.
 0.86614173 0.10629921 0.38976378 0.86614173 0.51968504 0.08661417
 0.         0.86614173 0.5511811  0.02755906 0.86614173 0.86614173
 0.00393701 0.         0.86614173 0.86614173 0.         0.86614173
 0.51574803 0.         0.58267717 0.86614173 0.         0.51181102
 0.86614173 0.         0.         0.77559055 0.         0.86614173
 0.12204724 0.86614173 0.         0.53149606 0.43307087 0.86614173
 0.         0.83070866 0.         0.86614173 0.78346457 0.
 0.86614173 0.         0.         0.14173228 0.74015748 0.
 0.         0.         0.         0.86614173 0.48818898 0.
 0.86614173 0.         0.27165354 0.86614173 0.86614173 0.86614173
 0.         0.80708661 0.         0.86614173 0.86614173 0.86614173
 0.18897638 0.86614173], y = [0. 0. 1. 0. 0.]
Sample 4: X = [0.76953125 0.8125     0.8203125  0.79296875 0.7578125  0.8125
 0.84765625 0.83203125 0.859375   0.90234375 0.99609375 1.
 0.8671875  0.8203125  0.78515625 0.80078125 0.76171875 0.8515625
 0.84765625 0.6796875  0.8828125  0.87890625 0.78515625 0.83984375
 0.74609375 0.76953125 0.74609375 0.80859375 0.78125    0.65625
 0.71875    0.79296875 0.765625   0.71484375 0.72265625 0.7265625
 0.80078125 0.6875     0.81640625 0.80859375 0.65625    0.79296875
 0.75390625 0.6796875  0.69921875 0.78515625 0.8125     0.7734375
 0.80078125 0.76953125 0.76953125 0.734375   0.8046875  0.66796875
 0.75390625 0.75       0.73828125 0.74609375 0.74609375 0.734375
 0.76953125 0.7578125  0.73828125 0.78125    0.76171875 0.7734375
 0.78125    0.7109375  0.765625   0.74609375 0.7265625  0.73046875
 0.63671875 0.671875   0.734375   0.69140625 0.703125   0.71484375
 0.671875   0.70703125 0.75       0.73046875 0.62109375 0.625
 0.68359375 0.69140625 0.60546875 0.671875   0.5625     0.66015625
 0.6796875  0.7265625  0.66015625 0.67578125 0.703125   0.75
 0.703125   0.625      0.64453125 0.6640625  0.67578125 0.62890625
 0.69140625 0.66015625 0.5625     0.62890625 0.59765625 0.6796875
 0.703125   0.671875   0.66015625 0.671875   0.703125   0.6328125
 0.66796875 0.65234375 0.63671875 0.546875   0.6875     0.625
 0.59765625 0.484375   0.62109375 0.63671875 0.578125   0.609375
 0.61328125 0.60546875 0.5234375  0.59765625 0.60546875 0.62890625
 0.59765625 0.640625   0.59765625 0.62109375 0.60546875 0.5703125
 0.61328125 0.5625     0.5234375  0.56640625 0.59765625 0.52734375
 0.4765625  0.625      0.6171875  0.578125   0.49609375 0.578125
 0.59765625 0.5625     0.57421875 0.625      0.58984375 0.56640625
 0.5703125  0.58984375 0.5859375  0.5390625  0.5546875  0.515625
 0.49609375 0.59375    0.59765625 0.53515625 0.4453125  0.53125
 0.5078125  0.54296875 0.55859375 0.49609375 0.54296875 0.5078125
 0.4765625  0.484375   0.57421875 0.57421875 0.54296875 0.55859375
 0.4765625  0.515625   0.5390625  0.5625     0.4765625  0.48046875
 0.50390625 0.48828125 0.52734375 0.5546875  0.43359375 0.4453125
 0.5234375  0.546875   0.5078125  0.52734375 0.53125    0.51171875
 0.49609375 0.5234375  0.4375     0.484375   0.52734375 0.5390625
 0.5        0.4921875  0.53515625 0.53125    0.48046875 0.53125
 0.46484375 0.3828125  0.51953125 0.48046875 0.46875    0.4453125
 0.42578125 0.47265625 0.44921875 0.48828125 0.46875    0.47265625
 0.484375   0.42578125 0.46875    0.4375     0.45703125 0.46875
 0.41015625 0.45703125 0.4375     0.48828125 0.49609375 0.4140625
 0.41015625 0.45703125 0.44921875 0.4296875  0.42578125 0.44140625
 0.4609375  0.4609375  0.47265625 0.48046875 0.41015625 0.45703125
 0.359375   0.421875   0.41796875 0.453125   0.46484375 0.39453125
 0.4296875  0.4140625  0.43359375 0.47265625 0.47265625 0.
 0.859375   0.59765625 0.         0.         0.859375   0.5
 0.859375   0.         0.05078125 0.859375   0.         0.
 0.5        0.         0.         0.859375   0.265625   0.859375
 0.859375   0.16796875 0.75390625 0.859375   0.30859375 0.80859375
 0.         0.03125    0.859375   0.4140625  0.55859375 0.859375
 0.30078125 0.859375   0.10546875 0.         0.859375   0.859375
 0.         0.859375   0.         0.30859375 0.859375   0.
 0.859375   0.1875     0.859375   0.2578125  0.859375   0.
 0.         0.859375   0.36328125 0.         0.859375   0.
 0.8359375  0.734375   0.         0.         0.859375   0.
 0.52734375 0.859375   0.203125   0.859375   0.17578125 0.859375
 0.859375   0.         0.859375   0.44921875 0.859375   0.75390625
 0.         0.859375   0.         0.859375   0.         0.19921875
 0.859375   0.         0.         0.859375   0.63671875 0.
 0.859375   0.5703125  0.         0.22265625 0.         0.859375
 0.703125   0.0390625  0.07421875 0.859375   0.54296875 0.
 0.40234375 0.         0.859375   0.32421875 0.         0.859375
 0.         0.84375    0.2421875  0.         0.859375   0.
 0.6953125  0.         0.859375   0.05859375 0.53515625 0.
 0.         0.859375   0.83203125 0.         0.         0.
 0.859375   0.15234375 0.859375   0.859375   0.1640625  0.859375
 0.         0.         0.859375   0.         0.7734375  0.859375
 0.         0.1640625  0.859375   0.859375   0.37109375 0.
 0.71875    0.         0.6015625  0.859375   0.         0.
 0.859375   0.69140625 0.2265625  0.         0.859375   0.4296875
 0.         0.453125   0.859375   0.859375   0.66796875 0.
 0.859375   0.         0.58203125 0.         0.84765625 0.
 0.578125   0.         0.859375   0.859375   0.55859375 0.
 0.859375   0.6328125  0.         0.859375   0.78125    0.859375
 0.         0.859375   0.390625   0.         0.3046875  0.859375
 0.2421875  0.859375   0.15234375 0.63671875 0.859375   0.25390625
 0.734375   0.         0.5234375  0.85546875 0.4921875  0.
 0.296875   0.859375   0.37109375 0.         0.         0.859375
 0.22265625 0.         0.859375   0.53515625 0.         0.859375
 0.         0.859375   0.41015625 0.         0.35546875 0.859375
 0.78515625 0.         0.         0.         0.859375   0.33984375
 0.859375   0.859375   0.         0.36328125 0.         0.68359375
 0.859375   0.3828125  0.4375     0.859375   0.         0.59375
 0.         0.859375   0.37890625 0.859375   0.         0.3125
 0.         0.8515625  0.859375   0.         0.2109375  0.
 0.75390625 0.859375   0.859375   0.         0.859375   0.859375
 0.         0.62109375 0.         0.859375   0.4921875  0.00390625
 0.         0.859375  ], y = [0. 0. 0. 0. 1.]
Sample 5: X = [0.78346457 0.82283465 0.82677165 0.79527559 0.78740157 0.83464567
 0.86220472 0.81102362 0.88582677 0.90944882 1.         1.
 0.9015748  0.83070866 0.81496063 0.83858268 0.86614173 0.81889764
 0.86220472 0.8503937  0.88188976 0.90944882 0.7992126  0.88582677
 0.81496063 0.7992126  0.70472441 0.70866142 0.73228346 0.80708661
 0.7992126  0.76771654 0.58267717 0.73622047 0.75984252 0.78346457
 0.68503937 0.80708661 0.78346457 0.75590551 0.73622047 0.70866142
 0.77559055 0.78346457 0.75984252 0.75590551 0.7480315  0.74015748
 0.74015748 0.67716535 0.74409449 0.68897638 0.7007874  0.69685039
 0.73622047 0.75590551 0.66141732 0.66929134 0.67322835 0.70866142
 0.63385827 0.62204724 0.72440945 0.68110236 0.57480315 0.64566929
 0.70866142 0.7007874  0.66929134 0.5984252  0.61023622 0.58661417
 0.6496063  0.63779528 0.6496063  0.65354331 0.65354331 0.65354331
 0.61811024 0.66535433 0.68897638 0.65748031 0.59448819 0.52362205
 0.53937008 0.59448819 0.65748031 0.64173228 0.65354331 0.58267717
 0.5984252  0.63385827 0.56692913 0.4488189  0.6023622  0.5511811
 0.48818898 0.61417323 0.55511811 0.58267717 0.57874016 0.53937008
 0.61023622 0.55905512 0.56692913 0.53149606 0.52755906 0.47244094
 0.50393701 0.51574803 0.56692913 0.55905512 0.54724409 0.46456693
 0.5984252  0.58661417 0.5511811  0.55511811 0.51181102 0.56299213
 0.48425197 0.51968504 0.47637795 0.55511811 0.55511811 0.53543307
 0.51574803 0.53937008 0.54724409 0.53937008 0.55511811 0.53937008
 0.5        0.41732283 0.50787402 0.50787402 0.48818898 0.35826772
 0.45275591 0.48818898 0.41732283 0.41338583 0.49606299 0.4488189
 0.50787402 0.46850394 0.48425197 0.50787402 0.43700787 0.46456693
 0.40944882 0.41338583 0.47244094 0.45669291 0.46456693 0.43307087
 0.43700787 0.43307087 0.4015748  0.42519685 0.42125984 0.42125984
 0.40944882 0.43307087 0.41732283 0.44488189 0.44094488 0.38976378
 0.44488189 0.42913386 0.43700787 0.48818898 0.43307087 0.42125984
 0.44094488 0.45275591 0.37401575 0.40944882 0.37401575 0.41338583
 0.44094488 0.46456693 0.38582677 0.34645669 0.43700787 0.37795276
 0.42519685 0.37007874 0.42519685 0.35826772 0.42913386 0.41732283
 0.41732283 0.4015748  0.3976378  0.37795276 0.40551181 0.40944882
 0.44094488 0.42519685 0.43307087 0.42913386 0.38582677 0.42519685
 0.49606299 0.46456693 0.48818898 0.41732283 0.35826772 0.40551181
 0.40551181 0.3503937  0.3976378  0.35433071 0.34251969 0.38188976
 0.37795276 0.40551181 0.31889764 0.3976378  0.39370079 0.36220472
 0.33858268 0.33858268 0.42519685 0.43307087 0.38976378 0.38188976
 0.39370079 0.40551181 0.3976378  0.37401575 0.35826772 0.37795276
 0.41338583 0.4015748  0.38582677 0.41338583 0.38188976 0.38976378
 0.37007874 0.3976378  0.36614173 0.35826772 0.37795276 0.4015748
 0.40551181 0.37007874 0.38188976 0.37401575 0.37401575 0.4015748
 0.4015748  0.41732283 0.35826772 0.47244094 0.45669291 0.
 0.86614173 0.74409449 0.14566929 0.         0.86614173 0.
 0.86614173 0.         0.04330709 0.86614173 0.         0.
 0.05511811 0.86614173 0.         0.         0.83858268 0.86614173
 0.         0.         0.86614173 0.86614173 0.         0.
 0.86614173 0.07086614 0.         0.55511811 0.86614173 0.
 0.         0.86614173 0.         0.86614173 0.59055118 0.
 0.69291339 0.         0.86614173 0.         0.86614173 0.23622047
 0.         0.86614173 0.36220472 0.86614173 0.         0.86614173
 0.81102362 0.         0.         0.40944882 0.         0.77559055
 0.         0.         0.         0.86614173 0.         0.86614173
 0.24015748 0.86614173 0.         0.86614173 0.20866142 0.
 0.85826772 0.14566929 0.86614173 0.         0.70866142 0.
 0.86614173 0.35826772 0.86614173 0.         0.86614173 0.
 0.85826772 0.         0.86614173 0.44094488 0.59055118 0.86614173
 0.         0.86614173 0.32677165 0.86614173 0.         0.86614173
 0.         0.02362205 0.29133858 0.         0.         0.86614173
 0.         0.         0.86614173 0.23228346 0.23622047 0.86614173
 0.86614173 0.43700787 0.         0.         0.42125984 0.86614173
 0.86614173 0.38188976 0.59448819 0.         0.         0.83070866
 0.         0.84251969 0.42913386 0.         0.86614173 0.7007874
 0.86614173 0.         0.86614173 0.50393701 0.         0.
 0.86614173 0.         0.7519685  0.         0.71259843 0.
 0.86614173 0.00787402 0.86614173 0.         0.31102362 0.
 0.         0.86614173 0.86614173 0.03149606 0.         0.86614173
 0.86614173 0.02362205 0.         0.86614173 0.42519685 0.43307087
 0.         0.         0.70472441 0.03149606 0.86614173 0.
 0.         0.7992126  0.         0.86614173 0.86614173 0.
 0.86614173 0.14566929 0.86614173 0.         0.         0.86614173
 0.62992126 0.         0.44094488 0.7007874  0.         0.3503937
 0.         0.86614173 0.86614173 0.34251969 0.         0.86614173
 0.07086614 0.86614173 0.         0.86614173 0.77952756 0.86614173
 0.         0.72047244 0.86614173 0.         0.27952756 0.86614173
 0.         0.86614173 0.08267717 0.86614173 0.         0.24409449
 0.80708661 0.         0.47244094 0.3503937  0.29133858 0.03543307
 0.86614173 0.86614173 0.         0.         0.86614173 0.86614173
 0.         0.86614173 0.86614173 0.         0.86614173 0.09055118
 0.44488189 0.0984252  0.86614173 0.01181102 0.86614173 0.
 0.86614173 0.5984252  0.         0.86614173 0.86614173 0.
 0.44488189 0.86614173 0.         0.22834646 0.8503937  0.86614173
 0.         0.29527559 0.23228346 0.86614173 0.5984252  0.
 0.         0.62204724 0.86614173 0.86614173 0.16141732 0.86614173
 0.         0.86614173 0.         0.86614173 0.         0.27559055
 0.         0.86614173], y = [0. 0. 0. 1. 0.]</code></pre>
</div>
</div>
<p>This code manually saves and downloads the trained model for use later. Thjis block saves it on a local system in the working directory:</p>
<div id="cell-33" class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> load_model</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming you have trained and compiled your Keras model and stored it in a variable named 'model'</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: model = ... (your Keras model definition and training code)</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model - this will save the model locally if running on a local notebook in the working directory</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">'your_model-6.h5'</span>)  <span class="co"># Replace 'your_model.h5' with the desired file name</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The following code saves a local compiled model for using in tests along with a back up to save to my google colab directory.</p>
<div id="cell-35" class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Now, you can download the saved model to your local machine</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co">#The code below only works if you are running on the google colab notebook</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> files</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>files.download(<span class="st">'your_model.h5'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-36" class="cell" data-outputid="59d70bba-9892-452e-cd99-b6030a16dd40">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Mount Google Drive</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>drive.mount(<span class="st">'/content/gdrive'</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model to Google Drive</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">'/content/gdrive/MyDrive/your_model.h5'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount("/content/gdrive", force_remount=True).</code></pre>
</div>
</div>
</section>
</section>
<section id="now-to-test-the-model" class="level1">
<h1>Now to test the model!</h1>
<p>This is from chat GPT to compare the scan to the model.</p>
<p>To test a new list item against a trained Keras model, you need to follow these general steps:</p>
<p>Preprocess the New List Item: - Ensure that the new list item is preprocessed in the same way as your training data. This includes any normalization, scaling, or other preprocessing steps that were applied to the training data. Use the Trained Model to Make Predictions: - Load your pre-trained Keras model and use it to make predictions on the preprocessed new list item. The model’s predict function can be used for this. Interpret the Predictions: - Interpret the model’s predictions. If you are working on a classification task, the output of the predict function will be a probability distribution over the classes. You can take the class with the highest probability as the predicted class.</p>
<p>Here’s an example assuming you have already trained a model:</p>
<p>This line is where it takes in the new array. <strong>make sure you normalise any new array you feed into this</strong>:</p>
<p>new_list_item_array = np.array(np.array(X_test[40], dtype=np.int64))</p>
<div id="cell-40" class="cell" data-outputid="6e099c8c-1e29-403c-d113-06ec9e4a9fc5">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co">#THIS WORKS!!!!!!!!</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> load_model</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the pre-trained model</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> load_model(<span class="st">'your_model-6.h5'</span>)  <span class="co"># Replace 'your_model.h5' with the actual file path</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming 'new_list_item' is your new list item</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure to preprocess 'new_list_item' in the same way as your training data</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert 'new_list_item' to a NumPy array</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co"># I have added a conversion to turn the list into int64 datatype as I hope this will work better</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>new_list_item_array <span class="op">=</span> np.array(np.array(X_test[<span class="dv">40</span>], dtype<span class="op">=</span>np.int64))</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape the input to match the expected shape (batch_size, 512)</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>new_list_item_array <span class="op">=</span> new_list_item_array.reshape((<span class="dv">1</span>, <span class="dv">512</span>))</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict(new_list_item_array)</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpret the predictions (assuming a classification task)</span></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>predicted_class <span class="op">=</span> np.argmax(predictions)</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted Class:"</span>, predicted_class)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1/1 [==============================] - 0s 148ms/step
Predicted Class: 2</code></pre>
</div>
</div>
<p>The code below is a quick sanity check to make sure I have the right number of classifiers for identification. In this case there should be 5.</p>
<div id="cell-42" class="cell" data-outputid="bff791c6-b0e4-4114-cf7a-aeb044db8514">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> model.layers[<span class="op">-</span><span class="dv">1</span>].output_shape[<span class="dv">1</span>]</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of classes:"</span>, num_classes)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of classes: 5</code></pre>
</div>
</div>
<p>Below is a full belt and braces sanity check to make sure the array you put in the classifer model for checking is still the right kind of data!:</p>
<div id="cell-44" class="cell" data-outputid="41783430-c43e-4b2d-9692-dc308df716f6">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>plt.plot(lists2[<span class="dv">0</span>, :])</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Original Array'</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test[<span class="dv">40</span>])</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Normalized Array'</span>)</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2023jansrftrainingmodelWORKING_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>